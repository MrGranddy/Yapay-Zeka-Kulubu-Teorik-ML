{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bu Notebook'da Neler Var?\n",
    "\n",
    "Bu noktaya kadar basit bir lineer sistemi Least Squares Method ile nasıl kuracağımızı ve çözmek için kapalı form çözümü veya Gradient Descent'i nasıl kullanabileceğimizi tartıştık. Bu noktada Neural Network'lere girmeden hemen önce ihtiyacımız olacak başka bazı araçlara bakacağız ve yaptığımız optimizasyon işleminin görsel olarak ne anlama geldiğini biraz daha iyi anlamaya çalışacağız.\n",
    "\n",
    "Bu notebook'da yeri geldiğinde Neural Network'lerden de biraz örnek vermemiz gerekebilir, ama bunları şimdilik kapalı kutu örnekler olarak düşünün, daha sonra tam olarak nasıl çalıştıklarını detaylıca inceleyeceğiz.\n",
    "\n",
    "Özetlemek gerekirse iki ana odağımız var, convex ve convex olmayan optimizasyon problemleri için (terimleri araştırmak isterseniz [buradan](https://en.wikipedia.org/wiki/Convex_optimization) bakabilirsiniz) hata fonksiyonumuz parametre uzayında neye benziyor, nasıl görünüyor. Daha sonrasında ise yüksek miktarda datayı parça parça işleyebileceğimiz üzerinde duracağız bu da Stochastic Gradient Descent kısmı olacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Eğitmek ve Görselleştirmek için İhtiyacımız Olacak Fonksiyonlar\n",
    "\n",
    "Bu noktada işlerimizi kolaylaştırmak adına PyTorch kullanmaya geçebiliriz, aslında Neural Network'lerde gradyan hesabını henüz yapmadığımız için biraz erken olacak ama konu oraya gelince otomatik gradyan hesaplama kısmını kapatırız :) Şimdilik NumPy'ın biraz daha gelişmiş versiyonu olarak düşünebiliriz PyTorch'u.\n",
    "\n",
    "Öncelikle Least Squares Method örneğine PyTorch ile tekrar bakalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu noktada PyTorch'u kurmanız gerekiyor, eğer ne yaptığınızı çok bilmiyorsanız kesinlikle Anaconda kullanmanınızı öneririm.\n",
    "# Öbür türlü CUDA kütüphanelerini vs. kurmak ile bayağı bir uğraşmak gerekiyor, Anaconda ile tek bir komut ile bilgisayarınızın özelliklerine göre kurulum\n",
    "# yapabilirsiniz\n",
    "\n",
    "# Anaconda kurmak için: https://docs.anaconda.com/free/anaconda/install/index.html\n",
    "# PyTorch kurmak için: https://pytorch.org/get-started/locally/ (Doğru seçenekleri seçtiğinize emin olun :) Bilgisayarınızda NVIDIA bir ekran kartı varsa CUDA 11.7 veya 11.8 seçeneklerini kullanabilirsiniz, yoksa CPU seçin.)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yine rastgele bir eğitim datası ve karşılık gelen labelları oluşturalım:\n",
    "\n",
    "N = 10000 # Toplam veri sayısı\n",
    "D = 10   # Verilerin vektör uzunluğu\n",
    "\n",
    "X = torch.randn(N, D, device=device) # Gaussian Distribution'dan rastgele değerlere sahip bir matris oluşturuyoruz\n",
    "\n",
    "w = torch.randn(D, 1, device=device) # Gaussian Distribution'dan rastgele değerlere sahip bir matris oluşturuyoruz\n",
    "                                     # bunlar ile X matrisini çarpıp y değerlerini oluşturacağız daha sonra ise bu değerleri\n",
    "                                     # bilmiyor gibi davranarak X ve y ile w değerlerini bulmaya çalışacağız\n",
    "\n",
    "# Buradaki mantığın biraz daha temeline girmemiz gerekirse, Machine Learning'de X ve y'yi herhangi bir olay için ölçümler yaparak\n",
    "# elde ediyoruz, gerçek dünya problemlerinde \"w\" yani öğrenmeye çalıştığımız parametreler aslında ne olayı birebir temsil etmek için\n",
    "# yeterli, ne de ona dair pek bir fikrimiz var. Ama problemin yapısına bakarak bir model oluşturuyoruz ve X ile y arasındaki ilişki\n",
    "# şu şekilde bir modelle iyi kötü çözülür diyoruz, örneğin Least Squares Method'da bu 1 adet matris çarpımıydı ve o matrisin parametrelerini\n",
    "# bulmaya çalışıyorduk.\n",
    "\n",
    "# Rastgele ürettiğimiz bu datada elimizde ölçüm olmadığı için tersten gidiyoruz, w'yu biliyor gibi davranarak ondan y değerlerini oluşturuyoruz\n",
    "# bu sayede X ve y arasında gerçekten de öğrenilebilir bir ilişki olmasını garanti ediyoruz, çünkü ikisini de tamamen random üretseydik aralarında\n",
    "# öğrenilebilir bir ilişki olmayabilirdi (büyük ihtimalle de olmazdı), sonra tabii ki bizim asıl amacımız w'yu bulmak olduğu için onu bilmiyor gibi\n",
    "# davranıyoruz ve X ve y arasındaki ilişkiyi bulmaya çalışıyoruz.\n",
    "\n",
    "y = X @ w + torch.randn(N, 1, device=device) * 0.1 # X matrisini w ile çarpıp y değerlerini oluşturuyoruz\n",
    "                                                   # üstüne bir miktar daha gürültü eklememizin sebebi ise kusur olan ilişkiyi birazcık bozup kendi\n",
    "                                                   # işimizi zorlaştırmak :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Şimdi geçen haftalarda yaptığımız gibi bu problemi çözmeye çalışalım:\n",
    "\n",
    "# Kapalı Form Çözüm\n",
    "\n",
    "w_hat = torch.inverse(X.T @ X) @ X.T @ y # Kapalı form çözümü ile w_hat değerlerini buluyoruz\n",
    "\n",
    "print(\"Kapalı form çözümü ile w_hat değerleri:\\n\", w_hat.reshape(-1))\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "w_hat = torch.randn(D, 1, device=device) * 0.1 # w_hat değerlerini rastgele başlangıç değerleri ile başlatıyoruz (Gradient descentte değerleri ufak başlatmak çoğu zaman işimize gelir)\n",
    "\n",
    "lr = 0.01 # Learning rate değerimiz\n",
    "\n",
    "for t in range(1000): # 1000 epoch boyunca çalışacağız (epoch: tüm verileri bir kere görmek)\n",
    "\n",
    "    y_hat = X @ w_hat # y_hat değerlerini hesaplıyoruz\n",
    "    w_hat -= lr * ((2 * X.T @ (y_hat - y)) / N) # Gradient descent ile w_hat değerlerini güncelliyoruz\n",
    "\n",
    "print(\"Gradient descent ile w_hat değerleri:\\n\", w_hat.reshape(-1))\n",
    "print(\"w değerleri:\\n\", w.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gördüğünüz gibi buraya kadar değişen pek bir şey yok, şimdi bir de şu kendi kendine gradyan alma özelliği neymiş onu görelim, bunun için kendimize aynı problem için çok ufak bir neural network oluşturalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSMBasic(torch.nn.Module): # Bu Module vesaire işin OOP kısmı, torch.nn.Module'den türettiğimiz class'ın içinde kendi kendine türev almak ve başka bazı işler için hazır yapılar var\n",
    "\n",
    "    def __init__(self, D): # OOP bilmeyenler için kısaca, bir class'ın içindeki __init__ fonksiyonu o class'ı oluştururken çalışan fonksiyondur, özetle basit bir kurulum işlemi yapar\n",
    "        # self'in ne olduğu çok önemli değil, class içinde fonksiyon yaratırken başa self yazarız, detayları OOP dersinin konusu :)\n",
    "        # Sonrasında verdiğimiz her şey bizim kendi isteğimizle, mesela D'yi bir parametre olarak veriyoruz ki öğreneceği matrisin boyutunu bilsin\n",
    "        super().__init__() # Yine biraz OOP magic, bunu da başka bir class'ı extend ederken kullanıyoruz, \n",
    "\n",
    "        # Bu kısım kafanızda büyütmediğinizde çok basit, torch.nn.Linear dümdüz bizim gradient descent'i elimizle yaparken yaptığımız gibi\n",
    "        # D1'e D2 boyutlu bir matris oluşturur, artık kaçtan kaça gidiyorsak, bizim örneğimizde D'den 1'e gidiyoruz, yani Dx1 boyutlu bir matris oluşacak\n",
    "        # Neural Network'lerde layer dediğimiz şeyler kod açısında bu tür ufak ufak işlemler\n",
    "\n",
    "        self.linear = torch.nn.Linear(D, 1, bias=False) # Yukarıdaki w_hat = torch.randn(D, 1, device=device) * 0.1  ile hemen hemen aynı şeyi yapıyor, bias=False dememizin sebebini toplantıda tartışalım :)\n",
    "        # bir özelliği de bunu kullanırken matris çarpımı yapmayacağız, örneğin X @ w yapmak yerine self.linear(X) yapacağız\n",
    "        # O içerde aynı işlemi yapıp bize sonucu döndürecek\n",
    "\n",
    "    def forward(self, X): # PyTorch'da model oluştururken forward fonksiyonu bu modeli dümdüz çağırdığımızda yapılacak işlemi belirler, yani X'i alacak sonra ne yapacak bu model, hepsini buraya yazıyoruz\n",
    "\n",
    "        # bizim örneğimizde X'i alıp self.linear'a veriyoruz, o da matris çarpımı yapıp sonucu döndürüyor\n",
    "        out = self.linear(X)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artık bu modeli yaratıp işlerimizi ona yaptırabiliriz:\n",
    "\n",
    "model = LSMBasic(D).to(device) # Modelimizi yaratıyoruz, .to(device) kısmı da CPU'da mı GPU'da mı çalışacağını belirlemek için\n",
    "\n",
    "# Şimdi aynı gradient descent işlemini yapalım:\n",
    "\n",
    "lr = 0.01 # Learning rate değerimiz\n",
    "\n",
    "for t in range(1000): # 1000 epoch boyunca çalışacağız (epoch: tüm verileri bir kere görmek)\n",
    "\n",
    "    y_hat = model(X) # y_hat değerlerini hesaplıyoruz (Yukarıdaki ile tamamen aynı şeyi yaptığımızı anlamak için vakit harcamanız normal, iyice oturtursanız devamı daha da rahatlayacak)\n",
    "\n",
    "    loss = torch.mean((y_hat - y) ** 2) # Loss fonksiyonumuzu hesaplıyoruz, yani y_hat ile y arasındaki farkın karesinin ortalamasını alıyoruz (Yine farklı hiçbir şey yapmıyoruz, yukarda loss hesaplamadık çünkü gradyanı direkt biliyorduk)\n",
    "    # Gradyanımız hataya göre hesaplanacağı için burada hata hesaplayarak PyTorch'a gradyanı neye göre hesaplaması gerektiğini söylüyoruz\n",
    "\n",
    "    loss.backward() # İşte kritik kısım bu, bu fonksiyonu çağırdığımız anda PyTorch bizim elde ettiğimiz hatayı kullanarak gradyanı bizler için hesapladı, arkaplanda tutuyor\n",
    "\n",
    "    with torch.no_grad(): # Bunu şu an elimizle gradient descent yapacağımız için kullanıyoruz, basitçe PyTorch'a diyoruz ki buranın altında yapacağımız işlemler için gradyan hesaplama\n",
    "                          # Yoksa modeli eğittiğimiz işlem için gradyan hesaplar gradyanın gradyanı gibi garip bir şey olur, bu da PyTorch'ın kafasını karıştırır\n",
    "\n",
    "        for param in model.parameters(): # model.parameters() bize modelimizdeki tüm parametreleri veriyor, biz de bunları tek tek alıp güncelliyoruz (şimdilik bir tane var o da self.linear içindeki matris, ama ilerde değişecek :) )\n",
    "            param -= lr * param.grad # Gradient descent ile parametreleri güncelliyoruz (yukarıdaki ile birebir aynı şey)\n",
    "\n",
    "        model.zero_grad() # Gradientleri sıfırlıyoruz, çünkü bir sonraki epoch'ta tekrar hesaplayacağız, eğer sıfırlamazsak bir önceki epoch'ta hesapladığımız gradyanlar ile birikim olur\n",
    "\n",
    "print(\"PyTorch ile w_hat değerleri:\\n\", model.linear.weight.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eveet, farkındayım bir seferde çok fazla terim oldu, ama ilerde göreceksiniz ki bunlar çok standart işlemler, çoğu şeyi sadece PyTorch'un dilinden konuşmak için yapıyoruz, genelde de hep ama hep aynı şeyleri yapıyor olacağız. O yüzden anlamak için kendinizi çok yormanıza gerek yok, yüzeysel bir şekilde NumPy ile yaptığımız ile bunun arasındaki farkları anlamanız yeterli, yoksa super ne yapıyormuş, backward nasıl çalışıyormuş bunları çok düşünmeyin, yine toplantıda bir miktar tartışabiliriz :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi bununla kafamızı çok bulandırmadan görselleştirme meselesine biraz girelim. Öncelikle neyi görselleştiriyoruz bunu bir hatırlayalım. Artık hepimizin $X$ nedir $y$ nedir $w$ nedir bunları bildiğini düşünüyorum. Peki LSM'i (Least Squares Method'u) matematiksel olarak tanımlarken ne demiştik? Bir hata fonksiyonumuz var ve modelimizin parametrelerini girdi olarak alıyor, yani:\n",
    "\n",
    "$ E(w) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - w^T x_i)^2 $\n",
    "\n",
    "Evet şu $\\frac{1}{N}$ kısmını sonradan biraz ayak üstü ekledik, tek amacı loss değerimiz çok büyük olmasın da N tane örnek için hataların ortalaması gibi olsun. Çünkü değerin büyümesi matematiksel olarak sorun çıkarmasa da değerler çok büyüdüğünde numerik olarak bilgisayarı biraz bozuyor.\n",
    "\n",
    "Görselleştirmek istediğimiz şey tam olarak bu $E(w)$ fonksiyonu, çünkü biz aslında gradient descent ile sadece bu fonksiyonu optimize ediyoruz, X ve y değişmiyor. Ama burada bir sıkıntı var, $w$ $D$ boyutlu bir vektör, hatta ilerde birden fazla vektör ve matrisden oluşacak bir yapı. Yani en iyi ihtimalle onlarca parametreden oluşan bir yapıdan bahsediyoruz, 1 parametre için iş kolay, parametreyi x eksenine koy, hatayı y eksenine koy oldu, hadi 2 parametre için de ya 3D plot yapıyoruz ya da 2 boyutta hatayı renk ile göstererek bir şekilde hallediyoruz, ama 10 boyutu tam olarak görselleştirmemiz maalesef mümkün değil.\n",
    "\n",
    "Bu aslında başlı başına bir araştırma konusu, \"How to visualize error surface of neural networks\" diye aratırsanız bununla ilgili birçok makale bulabilirsiniz. Biz burada benim hoşuma giden ve açıkçası daha önce yazmayı denemediğim bir method kullanacağız, şöyle bakalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Grafik çizmek için matplotlib kütüphanesini kullanıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot z = x^2 + y^2\n",
    "# Bu kısım görselleştirme ile alakalı, meraklı değilseniz kod kısmını okumayabilirsiniz, basitçe z = x^2 + y^2 fonksiyonunu çiziyoruz\n",
    "\n",
    "fig = plt.figure() # Yeni bir grafik oluşturuyoruz\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d') # 3 boyutlu bir grafik oluşturuyoruz\n",
    "\n",
    "x = np.arange(-10, 10, 0.25) # x değerlerini oluşturuyoruz\n",
    "y = np.arange(-10, 10, 0.25) # y değerlerini oluşturuyoruz\n",
    "\n",
    "X, Y = np.meshgrid(x, y) # x ve y değerlerini birleştirip bir grid oluşturuyoruz\n",
    "\n",
    "Z = X ** 2 + Y ** 2 # z değerlerini hesaplıyoruz\n",
    "\n",
    "ax.plot_surface(X, Y, Z) # Grafikteki yüzeyi çiziyoruz\n",
    "\n",
    "plt.show() # Grafikleri gösteriyoruz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi 3 Boyutlu düşünemediğimizi farz edin, elimizde böyle bir yapı var, sürekli hata yüzeyi derken kastettiğim şey de bu, aslında 2 boyutlu bir yapı ama eğip bükmüşüz gibi, matematikse bu tip yapılara yüzey diyoruz, tabii 10 boyutlu bir uzaydan bahsediyorsak, yüzey dediğimiz şey 9 boyutlu da olabilir :) ama en nihayetinde yaşadığı uzayı doldurmayan ve matematiksel tanımı belli bir yapı (bakınız [manifold](https://en.wikipedia.org/wiki/Manifold)) (Surface diyorduk manifold nereden çıktı diyeceksiniz, şimdilik aynı şey gibi düşünün, detayları başka videonun konusu :) ).\n",
    "\n",
    "Konumuza dönersek, eğer sadece 2 boyutu anlayan bir canlı olsaydık bu yapıyı görselleştirmek için akıllıca bir çözüme ihtiyacımız olurdu. Bunlardan biri bu yüzey üstünden 2 boyutlu bir kesit almak, örneğin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot z = x^2 + y^2\n",
    "# Bu kısım görselleştirme ile alakalı, meraklı değilseniz kod kısmını okumayabilirsiniz, basitçe z = x^2 + y^2 fonksiyonunu çiziyoruz\n",
    "\n",
    "fig = plt.figure() # Yeni bir grafik oluşturuyoruz\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d') # 3 boyutlu bir grafik oluşturuyoruz\n",
    "\n",
    "x = np.arange(-10, 10, 0.25) # x değerlerini oluşturuyoruz\n",
    "y = np.arange(-10, 10, 0.25) # y değerlerini oluşturuyoruz\n",
    "\n",
    "X, Y = np.meshgrid(x, y) # x ve y değerlerini birleştirip bir grid oluşturuyoruz\n",
    "\n",
    "Z = X ** 2 + Y ** 2 # z değerlerini hesaplıyoruz\n",
    "\n",
    "ax.plot_surface(X, Y, Z) # Grafikteki yüzeyi çiziyoruz\n",
    "\n",
    "# X + Y = 0 düzlemine denk gelen kısmı kırmızı renkte çiziyoruz\n",
    "\n",
    "mask = np.abs(X + Y) < 0.4 # X + Y = 0 düzlemine denk gelen kısmı seçiyoruz (0.4 degeri tamamen deneme yanılma ile bulundu, direkt 0'a eşit nokta sayısı az diye daha güzel bir görsel elde etmek adına böyle oyunlar yaparız)\n",
    "\n",
    "ax.scatter3D(X[mask], Y[mask], Z[mask], c='r') # Seçtiğimiz kısmı çiziyoruz\n",
    "\n",
    "plt.show() # Grafikleri gösteriyoruz\n",
    "\n",
    "# Şimdi bu kesiti 2 boyutlu bir düzlemde çizelim:\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x_vals = np.linspace(-10, 10, 400)\n",
    "z_vals = 2 * x_vals**2\n",
    "\n",
    "plt.plot(x_vals, z_vals, color='red', label='Intersection of z=x^2+y^2 and x+y=0')\n",
    "plt.xlabel('xy')\n",
    "plt.ylabel('z')\n",
    "plt.title('Intersection of Surface with Plane')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabii düzlemi matematiksel olarak tanımlayıp izdüşüm bulmak ile uğraşmak oldukça zahmetli bir iş, o yüzden çok daha kolay bir yöntem var, x ve y için bir başlangıç ve bitiş noktası seçelim, örneğin:\n",
    "\n",
    "$ start = [-10, -10]$ ve $end = [10, 10] $\n",
    "\n",
    "Şimdi bu iki noktanın oluşturduğu doğru üstünde $n$ tane nokta seçelim, örneğin $n = 100$ diyelim. Bunun için önce bu iki nokta arasında bir adım vektörü yaratacağız, öyle ki başlangıçtan 100 adım attığımda sona ulaşacağız:\n",
    "\n",
    "$ step = \\frac{end - start}{n} $\n",
    "\n",
    "Artık x eksenine kaçıncı adımda olduğumuzu ve y eksenine de hata değerimizi yazabiliriz, ve bu yöntem parametrelerimiz kaç boyutlu olursa olsun çalışacak! Şimdi bunu kodlayalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot z = x^2 + y^2\n",
    "# Bu kısım görselleştirme ile alakalı, meraklı değilseniz kod kısmını okumayabilirsiniz, basitçe z = x^2 + y^2 fonksiyonunu çiziyoruz\n",
    "\n",
    "fig = plt.figure() # Yeni bir grafik oluşturuyoruz\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d') # 3 boyutlu bir grafik oluşturuyoruz\n",
    "\n",
    "x = np.arange(-10, 10, 0.25) # x değerlerini oluşturuyoruz\n",
    "y = np.arange(-10, 10, 0.25) # y değerlerini oluşturuyoruz\n",
    "\n",
    "X, Y = np.meshgrid(x, y) # x ve y değerlerini birleştirip bir grid oluşturuyoruz\n",
    "\n",
    "Z = X ** 2 + Y ** 2 # z değerlerini hesaplıyoruz\n",
    "\n",
    "ax.plot_surface(X, Y, Z) # Grafikteki yüzeyi çiziyoruz\n",
    "\n",
    "plt.show() # Grafikleri gösteriyoruz\n",
    "\n",
    "plt.figure() # Yeni bir grafik oluşturuyoruz\n",
    "\n",
    "start = np.array([-10.0, -10.0]) # Başlangıç noktamızı belirliyoruz\n",
    "end = np.array([10.0, 10.0]) # Bitiş noktamızı belirliyoruz\n",
    "step = (end - start) / 100 # Başlangıç ve bitiş noktaları arasındaki mesafeyi 100'e bölüp adımımızı belirliyoruz\n",
    "\n",
    "points = []\n",
    "\n",
    "for i in range(100): # 100 adım boyunca çalışacağız\n",
    "\n",
    "    # bu x ve y değerlerine karşılık gelen z değerini hesaplıyoruz\n",
    "    z = start[0] ** 2 + start[1] ** 2\n",
    "\n",
    "    points.append((i, z)) # Adım sayımız ve z değerimizi kaydediyoruz\n",
    "\n",
    "    start += step # Adımımızı ekliyoruz\n",
    "\n",
    "points = np.array(points) # Numpy array'e çeviriyoruz\n",
    "\n",
    "print(points.shape) # Numpy array'in boyutunu yazdırıyoruz\n",
    "\n",
    "# Şimdi bu kesiti 2 boyutlu bir düzlemde çizelim:\n",
    "plt.plot(points[:, 0], points[:, 1], color='red')\n",
    "plt.xlabel('xy (number of steps)')\n",
    "plt.ylabel('z')\n",
    "plt.title('Error Over Line')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi bu stratejiyi LSM problemini görselleştirmek için kullanalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_LSM(N, D):\n",
    "    X = torch.randn(N, D, device=device)\n",
    "    w = torch.randn(D, 1, device=device)\n",
    "    y = X @ w + torch.randn(N, 1, device=device) * 0.1\n",
    "    return X, y, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w = generate_random_LSM(10000, 10)\n",
    "\n",
    "# İki adet rastgele w belirledik, başlangıçtan bitişe yine aynı mantıkla adım adım yürüyüp hata değerlerini kaydedeceğiz\n",
    "w_start = torch.randn(10, 1, device=device) * 0.1\n",
    "w_end = torch.randn(10, 1, device=device) * 0.1\n",
    "\n",
    "step = (w_end - w_start) / 100\n",
    "\n",
    "points = []\n",
    "\n",
    "# Burada PyTorch modelimizi kullanmıyoruz çünkü bizim için işleri kolaylaştırıyor olsa da bir eksisi var\n",
    "# her şeyi bizim için hallettiği için birçok eleman arka planda ve ulaşması zor, örneğin burada modelin\n",
    "# içine girip parametreleri elle güncellemek gerekecek ve çok fazla kod karmaşası olacak anlamak için\n",
    "# buna bakalım sonra bir de direkt PyTorch'a müdahele etmemiz gerekse nasıl olacak ona bakalım\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    y_hat = X @ w_start\n",
    "    loss = torch.mean((y_hat - y) ** 2) # torch.mean toplayıp N'e bölmek yerine o işi bizim için yapıyor\n",
    "\n",
    "    points.append((i, loss.item()))\n",
    "\n",
    "    w_start += step\n",
    "\n",
    "points = np.array(points)\n",
    "\n",
    "plt.plot(points[:, 0], points[:, 1], color='red')\n",
    "plt.xlabel('w (number of steps)')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Error Over Line')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evet, sonuç olarak çıkan şey bize başta pek bir şey anlatmıyor gibi gözüküyor, bunun sebebi bulmaya çalıştığımız optimum noktasının rastgele iki nokta arasında olmasının çok düşük olasılık olması :)\n",
    "Eğer güzel bir çanak şekli görmek istiyorsak asıl optimumun üstünden geçen bir doğru seçmemiz lazım, gelin bir de öyle deneyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w = generate_random_LSM(10000, 10)\n",
    "\n",
    "random_direction_vector = torch.randn(10, 1, device=device) * 0.1 # Rastgele bir yön vektörü oluşturuyoruz (daha sonra bu vektör ile optimumun iki ayrı ucunu başlangıc ve bitiş olarak seçeceğiz)\n",
    "\n",
    "w_start = w + random_direction_vector # Başlangıç noktamızı belirliyoruz (rastgele bir yönde bir miktar uzaklaştık w'den)\n",
    "w_end = w - random_direction_vector # Bitiş noktamızı belirliyoruz (bu sefer tam ters yöne gittik)\n",
    "# Şu an optimum noktamız w_start ve w_end'in ortasında\n",
    "\n",
    "step = (w_end - w_start) / 100\n",
    "\n",
    "points = []\n",
    "\n",
    "# Burada PyTorch modelimizi kullanmıyoruz çünkü bizim için işleri kolaylaştırıyor olsa da bir eksisi var\n",
    "# her şeyi bizim için hallettiği için birçok eleman arka planda ve ulaşması zor, örneğin burada modelin\n",
    "# içine girip parametreleri elle güncellemek gerekecek ve çok fazla kod karmaşası olacak anlamak için\n",
    "# buna bakalım sonra bir de direkt PyTorch'a müdahele etmemiz gerekse nasıl olacak ona bakalım\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    y_hat = X @ w_start\n",
    "    loss = torch.mean((y_hat - y) ** 2) # torch.mean toplayıp N'e bölmek yerine o işi bizim için yapıyor\n",
    "\n",
    "    points.append((i, loss.item()))\n",
    "\n",
    "    w_start += step\n",
    "\n",
    "points = np.array(points)\n",
    "\n",
    "plt.plot(points[:, 0], points[:, 1], color='red')\n",
    "plt.xlabel('w (number of steps)')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Error Over Line')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İşte şimdi güzel bir görsel elde ettik. Ben burada direkt bilmediğimizi kabul ettiğimiz w'yu kullandım ama normalde tabii ki optimal w için eğittiğimiz modelin parametrelerini kullanmamız gerekir. Peki o zaman zaten çözülmemiş bir şeyi görselleştiremiyoruz :D Aslında bu beklenen bir şey, çünkü çözülmemiş bir modeli bu şekilde görselleştirebilseydik çözümün nerede olduğunu görürdük, o zaman zaten bu kadar işi yapmamıza gerek olmazdı, yani görselleştirmeyi problemi çözmek için değil anlamak için yapıyoruz.\n",
    "Bu arada gördüğünüz gibi size bana güvenin demiştim, hata yüzeyimiz en azından bu açıdan bakınca tam bir çanak şeklinde yani convex, artık diğer yönlerden bakmak için elinizde bir araç var, isterseniz deneyin :) (Rastgele yön vektörünü değiştirerek farklı açılardan izdüşüme bakabilirsiniz.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi Stochastic Gradient Descent'ten bahsedelim, burada genel mantığı anlamaya çalışacağız. Momentum, Adam, RMSProp gibi daha gelişmiş yöntemleri ise bir okuma olarak bırakacağım, ama kısaca arkalarındaki motivasyondan bahsederiz.\n",
    "\n",
    "Tüm mesele elimizdeki veri sayısından çıkıyor, evet $N$ 10000 olduğu zaman datamızı parçalara bölmeye gerek duymuyoruz, ama $N$ 100 milyon olduğu zaman durum değişiyor, artık her bir adımda datamızı parça parça işlememiz gerekiyor, burada ben kendi bilgisayarımı kırmamak için örnek kod koymuyorum ama yukarıdaki örneklerde N'i 100M gibi büyük bir değer yaparak sonuçları görmeyi deneyebilirsiniz :) (Bilgisayar takılır en fazla, bir şey olmaz :D)\n",
    "\n",
    "Stochastic Gradient Descent'i iyi anlamak için problemimizi biraz değiştireceğiz, şimdi hata fonksiyonumuzu tekrar yazalım:\n",
    "\n",
    "$ E(w) = \\mathbb{E}_{(x, y) \\sim P}[ (y - w^T x)^2 ] $\n",
    "\n",
    "Eveet, istatistik ve olasılıktan tanıdığımız veya tanımıyorsak da şimdi tanıyacağımız $\\mathbb{E}$ yani \"estimated value\". Burada $P$ bir olasılık dağılımı, peki kısaca ve insan diliyle bu ifade ne anlama geliyor açıklayalım:\n",
    "\n",
    "$x \\in \\mathbb{R}^D$ ve $y\\in \\mathbb{R}$ öyle birer ikilidir ki, $P$ olasılık dağılımdan gelirler, burada $P(x, y)$ bize $(x, y)$ ikilisi ile karşılaşma olasılığımızı verir. Aslında bizim şu ana kadar yaptığımız iş, bu hayali dağılımdan $N$ tane örnek çekmekti, yani halihazırda elimizde olmayan bir şeyi az örnek ile tahmin etmeye çalışıyorduk. Şimdi ise farklı bir şey yapmayacağız, sadece elimizdeki örneği de parçalara ayırıp her bir parça için problemimizi ayrı ayrı çözeceğiz. Yani elimizdeki datayı $K$ parçaya bölüp, sanki elimizde $K$ adet problem varmış gibi davranacağız, her problemi çözdüğümüzde de parametrelerimizi güncelleyeceğiz. Tabii bu yüzden her parçamız için hata yüzeyimiz değişecek, çünkü farklı X ve y'ler kullanıyor olacağız, ama varsayımımız şu, tüm bu data zaten aynı dağılımdan geliyorsa, hepsinin hata yüzeyi aşağı yukarı benzer olmalı, o yüzden bu işi yapabiliyoruz. Üstüne üstlük her bir parçadaki ufak tefek farklılıklar da olur da local bir optimuma takılırsak bir sonraki parçada oradan kurtulmamızı sağlayacak! (Bununla ilgili birçok soru işareti oluşmuş olabilir bunun için iki şey yapalım: 1. https://en.wikipedia.org/wiki/Monte_Carlo_method buraya bakın. Aslında her adımda bu kusursuz ve ulaşamadığımız dağılımdan gelecek gradyanın Monte Carlo tahminini yapmaya çalışıyoruz. 2. Bu konuyu toplantıda tartışalım :) )\n",
    "\n",
    "Ama şimdilik bu kadar açıklamanın kafanıza yattığını düşünüyorum, gelin deneyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Öncelikle bir batch size belirlememiz gerekiyor, bu verimizi kaçlık parçalara böleceğimizi belirliyor\n",
    "\n",
    "batch_size = 1000 # Tam sayı olsun ki işimiz kolay olsun :)\n",
    "model = LSMBasic(D).to(device) # Modelimizi yaratıyoruz\n",
    "lr = 0.001 # Learning rate değerimiz\n",
    "\n",
    "# Şimdi verimizi batch_size'lık parçalara bölelim bunu yapmadan önce veriyi şöyle bir karıştırmak her zaman iyi bir fikirdir\n",
    "# Burada birtakım numaralar yapmamız gerekiyor, hem NumPy hem PyTorch'da şunu yapabiliyoruz:\n",
    "\n",
    "for i in range(1000): # 1000 epoch yapacağız :) Şimdi neden buna epoch diye ayrı bir isim verildiğini anlayacaksınız, her batch bir iteration, tüm batchleri bir kere gezmek de bir epoch\n",
    "    perm = torch.randperm(10000) # 0'dan 10000'e kadar rastgele sayılar oluşturuyoruz (Sonra X'imizin 0'dan 10000'e kadar olan satırlarını yeni satırlara tayin edeceğiz, bu sayede sıraları karışacak)\n",
    "    X = X[perm, :] # X'in iki ekseni var biri N için biri D için, N için olana 0 dan 10000'a yeni sıralar verdik, D için olana : diyerek bunu olduğu gibi bırak dedik\n",
    "                # Bunlar genel olarak Python'da matematik işlemlerinde kullandığımız bazı kısaltmalar, mesela X[:, 0] dersek X'in tüm satırlarını alıp 0. sütununu alırız gibi gibi\n",
    "                # Buna ayrıca çalışmak isterseniz NumPy vectorization ve broadcasting diye aratırsanız bulabilirsiniz\n",
    "\n",
    "    y = y[perm, :] # y için de aynı şeyi yapıyoruz\n",
    "\n",
    "    # Şimdi veriyi parçalara bölelim\n",
    "    batches = []\n",
    "    for j in range(0, N, batch_size):\n",
    "        batches.append((X[j:j + batch_size, :], y[j:j + batch_size, :])) # X ve y'yi batch_size'lık parçalara bölüyoruz ve batches listesine ekliyoruz\n",
    "\n",
    "    # Şimdi batches listesindeki her bir elemanı alıp onunla işlem yapacağız\n",
    "    for batch in batches:\n",
    "        X_batch, y_batch = batch\n",
    "\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= lr * param.grad\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "        # Tamamen aynı şeyi yaptık, sadece veriyi parçalara böldük ve her bir parçayı ayrı ayrı işledik\n",
    "\n",
    "    if (i + 1) % 100 == 0: # Her 100 epoch'ta bir loss değerimizi yazdırıyoruz\n",
    "        print(\"Epoch:\", i + 1, \"Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# Sonunda da bulduğumuz w_hat değerlerini yazdırıyoruz\n",
    "print(\"PyTorch ile w_hat değerleri:\\n\", model.linear.weight.reshape(-1))\n",
    "print(\"w değerleri:\\n\", w.reshape(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gördüğünüz gibi çok iyi çalışıyor :) Şimdi aynı şeyi gerçekten de tek seferde hesaplayamayacağımız kadar büyük bir verisetinde deneyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "D = 10\n",
    "\n",
    "X, y, w = generate_random_LSM(N, D)\n",
    "\n",
    "# Öncelikle bir batch size belirlememiz gerekiyor, bu verimizi kaçlık parçalara böleceğimizi belirliyor\n",
    "batch_size = 10000 # Tam sayı olsun ki işimiz kolay olsun :)\n",
    "model = LSMBasic(D).to(device) # Modelimizi yaratıyoruz\n",
    "lr = 0.001 # Learning rate değerimiz\n",
    "\n",
    "# Şimdi verimizi batch_size'lık parçalara bölelim bunu yapmadan önce veriyi şöyle bir karıştırmak her zaman iyi bir fikirdir\n",
    "# Burada birtakım numaralar yapmamız gerekiyor, hem NumPy hem PyTorch'da şunu yapabiliyoruz:\n",
    "\n",
    "for i in range(500): # Şimdi neden buna epoch diye ayrı bir isim verildiğini anlayacaksınız, her batch bir iteration, tüm batchleri bir kere gezmek de bir epoch\n",
    "    perm = torch.randperm(N) # 0'dan 10000'e kadar rastgele sayılar oluşturuyoruz (Sonra X'imizin 0'dan 10000'e kadar olan satırlarını yeni satırlara tayin edeceğiz, bu sayede sıraları karışacak)\n",
    "    X = X[perm, :] # X'in iki ekseni var biri N için biri D için, N için olana 0 dan 10000'a yeni sıralar verdik, D için olana : diyerek bunu olduğu gibi bırak dedik\n",
    "                # Bunlar genel olarak Python'da matematik işlemlerinde kullandığımız bazı kısaltmalar, mesela X[:, 0] dersek X'in tüm satırlarını alıp 0. sütununu alırız gibi gibi\n",
    "                # Buna ayrıca çalışmak isterseniz NumPy vectorization ve broadcasting diye aratırsanız bulabilirsiniz\n",
    "\n",
    "    y = y[perm, :] # y için de aynı şeyi yapıyoruz\n",
    "\n",
    "    # Şimdi veriyi parçalara bölelim\n",
    "    batches = []\n",
    "    for j in range(0, N, batch_size):\n",
    "        batches.append((X[j:j + batch_size, :], y[j:j + batch_size, :])) # X ve y'yi batch_size'lık parçalara bölüyoruz ve batches listesine ekliyoruz\n",
    "\n",
    "    # Şimdi batches listesindeki her bir elemanı alıp onunla işlem yapacağız\n",
    "    for batch in batches:\n",
    "        X_batch, y_batch = batch\n",
    "\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= lr * param.grad\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "        # Tamamen aynı şeyi yaptık, sadece veriyi parçalara böldük ve her bir parçayı ayrı ayrı işledik\n",
    "\n",
    "    if (i + 1) % 100 == 0: # Her 100 epoch'ta bir loss değerimizi yazdırıyoruz\n",
    "        print(\"Epoch:\", i + 1, \"Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# Sonunda da bulduğumuz w_hat değerlerini yazdırıyoruz\n",
    "print(\"PyTorch ile w_hat değerleri:\\n\", model.linear.weight.reshape(-1))\n",
    "print(\"w değerleri:\\n\", w.reshape(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorun yaşadıysanız N değerinden 1 adet 0 silmek işe yarayabilir, yine burada göstermelik olarak aslında tüm veriyi tek seferde RAM'e yüklüyoruz ürettiğimiz anda, ama gerçek hayatta bu veri çok daha büyük olacak ve hard disk'te olacak, o yüzden parçalara bölmeden tüm veriyi de yüklemeyeceğiz, hard disk'ten de parça parça okuyarak her türlü hafıza probleminin önüne geçeceğiz :)\n",
    "\n",
    "Farklı gradient descent yöntemleri için bir kaynak:\n",
    "\n",
    "https://www.ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "Şu aşamada her şeyi tam olarak anlamanıza gerek olmayabilir, aslında bu konuya detaylı girmeme sebebimiz de biraz bu, biraz daha giriş seviyesinde tutmak. En nihayetinde bunların her biri, istatistiksel olarak gradient descent'in saçma yerlere sapmaması veya olabildiğince hızlı bir şekilde optimuma ulaşması için yapılmış değişiklikler. Belki ilerde biraz daha detaylı bir şekilde bakarız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi iki olayı birleştirip gerçekten de her bir batch için oluşturduğumuz hata yüzeyleri benziyor mu bakalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Şimdi aynı veri setimizi alalım, yine batchlere bölelim ama bu sefer batchleri görselleştirme algoritmamıza verelim\n",
    "\n",
    "# Optimal w'yu bulmuştuk, gelin yine başlangıç ve bitiş noktalarını belirleyelim\n",
    "\n",
    "model_w = model.linear.weight.clone().detach() # Modelin içindeki w değerlerini kopyalıyoruz (orijinalini yanlışlıkla değiştirmemek için)\n",
    "print(model_w)\n",
    "\n",
    "print(f\"Modelin içinde w'nun şekli: {model_w.shape}\")\n",
    "\n",
    "# Bu sefer birazcık şımarıp söz verdiğim gibi her şeyi modelin içindeki parametreleri kullanarak yapacağız\n",
    "# Dikkatinizi çekmesi gereken nokta PyTorch matrisi 10x1 olarak değil 1x10 olarak tutmuş, bunun özel bir sebebi yok\n",
    "# en nihayetinde aynı işlemi yapıyor olacak, sonuçta X @ w ile w.T @ X.T aynı şey (çünkü sonuç skalar)\n",
    "# Yani bu shapelerin tek başına bir önemi yok, her şey problemi nasıl tasarladığımıza bağlı :)\n",
    "\n",
    "random_direction_vector = torch.randn(1, 10, device=device) # Rastgele bir yön vektörü oluşturuyoruz (daha sonra bu vektör ile optimumun iki ayrı ucunu başlangıc ve bitiş olarak seçeceğiz)\n",
    "\n",
    "w_start = model_w + random_direction_vector # Başlangıç noktamızı belirliyoruz (rastgele bir yönde bir miktar uzaklaştık w'den)\n",
    "w_end = model_w - random_direction_vector # Bitiş noktamızı belirliyoruz (bu sefer tam ters yöne gittik)\n",
    "\n",
    "# Şu an optimum noktamız w_start ve w_end'in ortasında\n",
    "\n",
    "step = (w_end - w_start) / 100\n",
    "\n",
    "# Şimdi verimizi yine batchlere bölelim\n",
    "\n",
    "batch_size = 10000\n",
    "batches = []\n",
    "\n",
    "# X'i bu sefer karıştırmıyorum, yeterince karıştırdık :) Ama eğitim yapacak olsam karıştırırdım\n",
    "for j in range(0, N, batch_size):\n",
    "    batches.append((X[j:j + batch_size, :], y[j:j + batch_size, :]))\n",
    "\n",
    "# İlk 5 batch için görselleştirme yapacağız\n",
    "\n",
    "points = []\n",
    "\n",
    "for batch_idx in range(5):\n",
    "\n",
    "    X_batch, y_batch = batches[batch_idx]\n",
    "\n",
    "    points.append([]) # Her batch için bir liste oluşturuyoruz\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        # Modelin içindeki parametreleri güncelliyoruz\n",
    "        model.linear.weight = torch.nn.Parameter(w_start + step * i)\n",
    "\n",
    "        # Güncellenmiş modelin sonuçları\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        # Loss fonksiyonumuz\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        # Hafızaya kaydediyoruz\n",
    "        points[batch_idx].append((i, loss.item()))\n",
    "\n",
    "points = np.array(points)\n",
    "\n",
    "# Modeli de eski haline getiriyoruz\n",
    "model.linear.weight = torch.nn.Parameter(model_w)\n",
    "\n",
    "# Şimdi de her bir batch'in grafiğini üst üste çizdirelim\n",
    "for batch_idx in range(5):\n",
    "    plt.plot(points[batch_idx, :, 0], points[batch_idx, :, 1], label=f\"Batch {batch_idx}\")\n",
    "\n",
    "plt.xlabel('w (number of steps)')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Error Over Line')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gördüğünüz gibi uçlara doğru farklılık gösterse de optimum noktaları neredeyse tamamen üst üste geliyor :) Buradan sonra bir de ufak bir Neural Network hata yüzeyi görselleştirmesi kısmı gelecek onu da pek yakında ekleyeceğim :) İyi çalışmalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi gelin küçük bir Neural Network ile aynı şeyleri yapalım, yine hata yüzeyini görselleştirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_dataset(N, D):\n",
    "    X = torch.randn(N, D, device=device)\n",
    "    w1 = torch.randn(D, 2 * D, device=device)\n",
    "    w2 = torch.randn(2 * D, 1, device=device)\n",
    "\n",
    "    a = torch.sigmoid(X) @ w1 + torch.randn(N, 1, device=device) * 0.1\n",
    "    a = a / torch.max(a) - 0.5\n",
    "    a = torch.sigmoid(a) @ w2 + torch.randn(N, 1, device=device) * 0.1\n",
    "    a = a / torch.max(a)\n",
    "\n",
    "    return X, a, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "D = 10\n",
    "\n",
    "X, y, w = generate_complex_dataset(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(D, 4 * D, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(4 * D, 2 * D, bias=False)\n",
    "        self.linear3 = torch.nn.Linear(2 * D, 1, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out = torch.relu(self.linear1(X))\n",
    "        out = torch.relu(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(D).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 10000\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    perm = torch.randperm(N)\n",
    "\n",
    "    X = X[perm, :]\n",
    "    y = y[perm, :]\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for j in range(0, N, batch_size):\n",
    "        batches.append((X[j:j + batch_size, :], y[j:j + batch_size, :]))\n",
    "\n",
    "    for batch in batches:\n",
    "\n",
    "        X_batch, y_batch = batch\n",
    "\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= lr * param.grad\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Epoch:\", i + 1, \"Loss:\", loss.item())\n",
    "\n",
    "print(\"PyTorch ile w_hat değerleri:\\n\", model.linear3.weight.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yine batchleri aynı şekilde görselleştirelim, bu sefer 3 ayrı matrisimiz olduğu için biraz daha fazla işimiz var\n",
    "\n",
    "ws = [model.linear1.weight.clone().detach(), model.linear2.weight.clone().detach(), model.linear3.weight.clone().detach()]\n",
    "\n",
    "random_direction_vectors = [torch.randn(4 * D, D, device=device), torch.randn(2 * D, 4 * D, device=device), torch.randn(1, 2 * D, device=device)]\n",
    "\n",
    "w_starts = [w + random_direction_vector * 2 for w, random_direction_vector in zip(ws, random_direction_vectors)]\n",
    "w_ends = [w - random_direction_vector * 2 for w, random_direction_vector in zip(ws, random_direction_vectors)]\n",
    "\n",
    "steps = [(w_end - w_start) / 100 for w_start, w_end in zip(w_starts, w_ends)]\n",
    "\n",
    "batches = []\n",
    "\n",
    "for j in range(0, N, batch_size):\n",
    "    batches.append((X[j:j + batch_size, :], y[j:j + batch_size, :]))\n",
    "\n",
    "# İlk 5 batch için görselleştirme yapacağız\n",
    "\n",
    "points = []\n",
    "\n",
    "for batch_idx in range(5):\n",
    "\n",
    "    X_batch, y_batch = batches[batch_idx]\n",
    "\n",
    "    points.append([]) # Her batch için bir liste oluşturuyoruz\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        # Modelin içindeki parametreleri güncelliyoruz\n",
    "        model.linear1.weight = torch.nn.Parameter(w_starts[0] + steps[0] * i)\n",
    "        model.linear2.weight = torch.nn.Parameter(w_starts[1] + steps[1] * i)\n",
    "        model.linear3.weight = torch.nn.Parameter(w_starts[2] + steps[2] * i)\n",
    "\n",
    "        # Güncellenmiş modelin sonuçları\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        # Loss fonksiyonumuz\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        # Hafızaya kaydediyoruz\n",
    "        points[batch_idx].append((i, loss.item()))\n",
    "\n",
    "points = np.array(points)\n",
    "\n",
    "# Modeli de eski haline getiriyoruz\n",
    "model.linear1.weight = torch.nn.Parameter(ws[0])\n",
    "model.linear2.weight = torch.nn.Parameter(ws[1])\n",
    "model.linear3.weight = torch.nn.Parameter(ws[2])\n",
    "\n",
    "# Şimdi de her bir batch'in grafiğini üst üste çizdirelim\n",
    "\n",
    "for batch_idx in range(5):\n",
    "    plt.plot(points[batch_idx, :, 0], points[batch_idx, :, 1], label=f\"Batch {batch_idx}\")\n",
    "\n",
    "plt.xlabel('w (number of steps)')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Error Over Line')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir de biraz zoom yapıp bakalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yine batchleri aynı şekilde görselleştirelim, bu sefer 3 ayrı matrisimiz olduğu için biraz daha fazla işimiz var\n",
    "\n",
    "ws = [model.linear1.weight.clone().detach(), model.linear2.weight.clone().detach(), model.linear3.weight.clone().detach()]\n",
    "\n",
    "random_direction_vectors = [torch.randn(4 * D, D, device=device), torch.randn(2 * D, 4 * D, device=device), torch.randn(1, 2 * D, device=device)]\n",
    "\n",
    "w_starts = [w + random_direction_vector * 0.01 for w, random_direction_vector in zip(ws, random_direction_vectors)]\n",
    "w_ends = [w - random_direction_vector * 0.01 for w, random_direction_vector in zip(ws, random_direction_vectors)]\n",
    "\n",
    "steps = [(w_end - w_start) / 100 for w_start, w_end in zip(w_starts, w_ends)]\n",
    "\n",
    "batches = []\n",
    "\n",
    "for j in range(0, N, batch_size):\n",
    "    batches.append((X[j:j + batch_size, :], y[j:j + batch_size, :]))\n",
    "\n",
    "# İlk 5 batch için görselleştirme yapacağız\n",
    "\n",
    "points = []\n",
    "\n",
    "for batch_idx in range(5):\n",
    "\n",
    "    X_batch, y_batch = batches[batch_idx]\n",
    "\n",
    "    points.append([]) # Her batch için bir liste oluşturuyoruz\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        # Modelin içindeki parametreleri güncelliyoruz\n",
    "        model.linear1.weight = torch.nn.Parameter(w_starts[0] + steps[0] * i)\n",
    "        model.linear2.weight = torch.nn.Parameter(w_starts[1] + steps[1] * i)\n",
    "        model.linear3.weight = torch.nn.Parameter(w_starts[2] + steps[2] * i)\n",
    "\n",
    "        # Güncellenmiş modelin sonuçları\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        # Loss fonksiyonumuz\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        # Hafızaya kaydediyoruz\n",
    "        points[batch_idx].append((i, loss.item()))\n",
    "\n",
    "points = np.array(points)\n",
    "\n",
    "# Modeli de eski haline getiriyoruz\n",
    "model.linear1.weight = torch.nn.Parameter(ws[0])\n",
    "model.linear2.weight = torch.nn.Parameter(ws[1])\n",
    "model.linear3.weight = torch.nn.Parameter(ws[2])\n",
    "\n",
    "# Şimdi de her bir batch'in grafiğini üst üste çizdirelim\n",
    "\n",
    "for batch_idx in range(5):\n",
    "    plt.plot(points[batch_idx, :, 0], points[batch_idx, :, 1], label=f\"Batch {batch_idx}\")\n",
    "\n",
    "plt.xlabel('w (number of steps)')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Error Over Line')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uğraştıysam da yüzeyde convex'liği bozan featureları bulamadım :) Ama bu en azından her ne kadar convex'liği bozmuş olsak da hata yüzeyimizin hala daha ne kadar convex'e yakın olduğunu gösteriyor.\n",
    "Bununla da bitirdik, bir sonraki notebook'ta görüşmek üzere :) Her kısımda oynanacak birçok kısım var, denemenizi tavsiye ederim."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
