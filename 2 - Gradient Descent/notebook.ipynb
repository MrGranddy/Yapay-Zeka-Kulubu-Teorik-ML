{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Nedir?\n",
    "\n",
    "Hatırlayalım, __least squares method__ ile çalışırken elimizdeki veriye göre hata fonksiyonumuzu minimize eden parametreleri bulmaya çalışıyorduk.\n",
    "\n",
    "Sonrasında birkaç adım sonucunda bize bu optimal parametreleri veren bir denklem elde ettik:\n",
    "\n",
    "$ w^* = (X^TX)^{-1}X^TY$\n",
    "\n",
    "Bu çözüm bize direkt olarak optimal parametreleri verdiği için __closed form__ olarak adlandırılır demiştik, fakat bu çözüm her zaman mümkün olmayabilir.\n",
    "Buna güzel bir örnek elimizde çok fazla veri olduğu durum, $ X \\in \\mathbb{R}^{N \\times D}$ olduğu durumda, çok fazla veri olması, $N$'in çok büyük olması anlamına geliyor.\n",
    "Bugün işe yarar bir veri setinden bahsettiğimizde $N$'in milyonlar hatta milyarlar seviyesinde büyük olduğunu düşünebiliriz, burada bahsetmediğimiz başka bazı ince detayları da\n",
    "düşündüğümüzde bu çözümün uygulanabilir olmadığını görebiliriz.\n",
    "\n",
    "O zaman bir alternatife ihtiyacımız var, öyle bir alternatif ki bizi sadece maaliyetli matris çarpımlarından kurtarmayacak, bir adım ötesinde hem hafızaya sığmayacak\n",
    "büyüklükte veri kümelerini kullanmak için bize bir yol sunacak hem de direkt olarak optimal parametreleri bulamayacağımız daha komplike hata fonksiyonlarını minimize etmekte\n",
    "hiçbir sorun yaşamayacak.\n",
    "\n",
    "Bu alternatif __Gradient Descent__. Şimdilik onu sadece bizi matris tersi gibi işlemlerden kurtarıyor gibi düşünelim.\n",
    "\n",
    "Gradient Descent, bir fonksiyonun minimum noktasına ulaşmak için kullanılan bir optimizasyon algoritmasıdır. Bu algoritma bizi direkt olarak minimum noktaya götürmez,\n",
    "ama aşama aşama bizi minimum noktasına yaklaştırır. Burada kendinize sorabileceğiniz güzel bir soru, bu algoritmanın çalışması için belli şartlar var mı? Bunu toplantılarda tartışabiliriz.\n",
    "\n",
    "Bu __notebook__'da Gradient Descent'in niye çalıştığı ile ilgili çok kısa bir matematiksel tanımın ardından bolca sezgisel örnek inceleyeceğiz.\n",
    "\n",
    "## Gradient Descent'in Matematiksel Tanımı\n",
    "\n",
    "Öncelikle Gradient yani gradyan nedir bunun üstünde durmamız lazım, aslında burada bahsedilecek pek bir şey yok, bir fonksiyonun belli bir noktadaki gradyanı bize o fonksiyonun\n",
    "en hızlı arttığı yönü verir. Bir dağın yüzeyini bir fonksiyon olarak düşünürseniz ve kendinizi de bu dağın üstünde hayal ederseniz, dağı modelleyen fonksiyonun sizin bulunduğunuz\n",
    "noktadaki gradyanı size 2 boyutlu bir vektör verir, bu vektör sizin hangi yönde 1 birim ilerlediğinizde en çok yükseleceğinizi gösteren vektördür. Kısaca en hızlı artış yönü diyebiliriz.\n",
    "\n",
    "Büyük ihtimalle nereye varıyor olduğumu anladınız, eğer gradyan yönünde gittiğimde yüksekliğim en hızlı artıyorsa, gradyanın tersi yönde gittiğimde de bu yüksekliğimin en hızlı azalacağı\n",
    "anlamına gelir. Bizim hata fonksiyonu dediğimiz fonksiyonu da bir dağ olarak düşünürsek, en dibe ulaşmak, hatanın en az olduğu optimumu noktaya ulaşmak demek, o yüzden bu yönde ilerlersek\n",
    "ulaşmak istediğimiz minimum noktasına ulaşabiliriz, ama tabii ki burada değerlendirmemiz gereken bazı şeyler var.\n",
    "\n",
    "Şimdi gelin birkaç örnek ile bu konuyu daha iyi anlamaya çalışalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Boyutlu Örnek\n",
    "\n",
    "İnternetteki kaynaklarda Gradient Descent ile ilgili çok güzel görselleştirmeler mevcut, fakat benim bunlarla ilgili en büyük sorunum her zaman şu oldu, 3 boyutlu örnekler bize biraz\n",
    "yanlış bir algı veriyor olabilir. Az önce bahsettiğim örneği düşünün, bir dağdan bahsettim, dağ 3 boyutlu bir obje ve gerçekten de baktığınızda ortada 3 boyutlu bir anlatım var, ama şu\n",
    "çok önemli. Dağ örneğinde yükseklik bizim için fonksiyonun sonucuydu, dağda yürüme örneği bizim yüksekliğimizi de bilinçli olarak değiştirdiğimiz yanılgısını veriyor ama bu örnekte gerçekte\n",
    "olan şey biz sadece 2 boyutlu hareket ediyoruz, öne arkaya, sağa ve sola daha sonra 2 boyutlu konumumuza göre fonksiyonumuz bize bir yükseklik tayin ediyor. Hatta daha anlamlı olması için şöyle diyelim\n",
    "biz sadece enlem ve boylamımızı değiştiriyoruz, yani dağ fonksiyonu şu şekilde gözükecek:\n",
    "\n",
    "$ f(enlem, boylam) = yükseklik $\n",
    "\n",
    "Bunun başta garip geldiğinin farkındayım, bana da öyle olmuştu. Garip olmasının sebebi gerçek 3 boyutlu bir dünyada biz sadece enlem ve boylamımızı değiştirmiyoruz aynı zamanda kendimizi yukarı taşıyoruz bu yüzden\n",
    "3 boyutlu bir hareket yapıyor oluyoruz. Yani bize Gradient Descent'i aslında oldukça iyi anlatan bu örnekte, bu ince ayrımın farkında olmamız gerekiyor. Şimdi 1 boyutlu örnekte demek istediğimi çok daha iyi anlayacaksınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir x^2 foknsiyonunu düşünelim.\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x ** 2\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada herhangi bir noktada gradyan aldığımızda, bu gradyanın şöyle gözüktüğünü düşünmeye meyilliyiz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir x^2 foknsiyonunu düşünelim.\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x ** 2\n",
    "\n",
    "x0 = 1\n",
    "y0 = x0 ** 2\n",
    "\n",
    "x_pseudo_grad = (0.5, x0 + y0)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.scatter(x0, y0, c=\"red\")\n",
    "plt.arrow(x0, y0, x_pseudo_grad[0], x_pseudo_grad[1], head_width=0.2, head_length=0.2, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ama aslında az önce açıkladığımız sebeplerden burada $f(x) = x^2$ fonksiyonunun x'e göre gradyanını aldığımız için aslında $y$ değeri üzerinde hiçbir kontrolümüz yok, biz sadece $x$ değerini değiştiriyoruz.\n",
    "\n",
    "O halde gerçek gradyan şöyle gözükecek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir x^2 foknsiyonunu düşünelim.\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x ** 2\n",
    "\n",
    "x0 = 1\n",
    "y0 = x0 ** 2\n",
    "\n",
    "x_grad = 2 * x0\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.scatter(x0, y0, c=\"red\")\n",
    "plt.arrow(x0, y0, x_grad, 0, head_width=0.2, head_length=0.2, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tam bu vektör kadar hareket ettiğimizde ise yeni noktamız şöyle olacak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir x^2 foknsiyonunu düşünelim.\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x ** 2\n",
    "\n",
    "x0 = 1\n",
    "y0 = x0 ** 2\n",
    "\n",
    "x_grad = 2 * x0\n",
    "\n",
    "x1 = x0 + x_grad\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.scatter(x0, y0, c=\"red\")\n",
    "plt.arrow(x0, y0, x_grad, 0, head_width=0.2, head_length=0.2, color=\"red\")\n",
    "plt.scatter(x1, x1 ** 2, c=\"green\")\n",
    "plt.plot([x1, x1], [0, x1 ** 2], \"--\", c=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Boyutlu Örnek\n",
    "\n",
    "Bu konsepti daha iyi oturtmak için şimdi bir de 2 boyutlu bir örneğe bakalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = x ** 2 + y ** 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(x, y, z)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = x ** 2 + y ** 2\n",
    "\n",
    "x0, y0 = -2, 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(x, y, z, alpha=0.8)\n",
    "ax.plot([x0 + 0.2], [y0 + 0.2], [x0 ** 2 + y0 ** 2 + 0.2], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu durumda gradyan da şöyle gözükecek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = x ** 2 + y ** 2\n",
    "\n",
    "x0, y0 = -2, 2\n",
    "\n",
    "grad = np.array([x0, y0, 0])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(x, y, z, alpha=0.8)\n",
    "ax.plot([x0 + 0.2], [y0 + 0.2], [x0 ** 2 + y0 ** 2 + 0.2], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "\n",
    "ax.quiver(x0 + 0.2, y0 + 0.2, x0 ** 2 + y0 ** 2 + 0.2, grad[0], grad[1], 0, color=\"red\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Görünüşü sizi yanılmasın, birazcık döndürürsek şöyle gözükecek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = x ** 2 + y ** 2\n",
    "\n",
    "x0, y0 = -2, 2\n",
    "\n",
    "grad = np.array([x0, y0, 0])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(40, 60)\n",
    "\n",
    "ax.plot_surface(x, y, z, alpha=0.8)\n",
    "ax.plot([x0 + 0.2], [y0 + 0.2], [x0 ** 2 + y0 ** 2 + 0.2], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "ax.quiver(x0 + 0.2, y0 + 0.2, x0 ** 2 + y0 ** 2 + 0.2, grad[0], grad[1], 0, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(0, 30)\n",
    "\n",
    "ax.plot_surface(x, y, z, alpha=0.8)\n",
    "ax.plot([x0 + 0.2], [y0 + 0.2], [x0 ** 2 + y0 ** 2 + 0.2], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "# Add 3D arrow\n",
    "ax.quiver(x0 + 0.2, y0 + 0.2, x0 ** 2 + y0 ** 2 + 0.2, grad[0], grad[1], 0, color=\"red\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aynı şeyi 2D bir plot kullanarak da şu şekilde gösterebiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = x ** 2 + y ** 2\n",
    "\n",
    "x0, y0 = -2, 2\n",
    "\n",
    "grad = np.array([2 * x0, 2 * y0, 0])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contour(x, y, z, 50, cmap=\"viridis\")\n",
    "plt.plot([x0], [y0], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "plt.quiver(x0, y0, grad[0], grad[1], color=\"red\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Böylece optimize ettiğimiz değeri sadece renk ve kontürler şeklinde görürken plotun kendisinde sadece hareket kabiliyetimiz olan yönleri görüyoruz ve aslında bu\n",
    "aklımıza daha çok yatacak bir görselleştirme oluyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent'i Nasıl Uyguylayacağız?\n",
    "\n",
    "Bu noktaya kadar aslına sadece gradyandan bahsettik, ama bu bile bize Gradient Descent algoritmasını nasıl kullanacağımız ile ilgili neredeyse\n",
    "tüm bilgiyi sağladı.\n",
    "\n",
    "Şimdi Gradient Descent'i nasıl yapabilirdik düşünelim, madem gradyan bize en hızlı artış yönünü ve gradyanın negatifi en hızlı düşüş yönünü gösteriyor,\n",
    "o zaman iteratif olarak negatif gradyan yönünde gidersek minimuma ulaşabiliriz. Bunu görselleştirelim (Bu sefer bir tık daha karışık bir fonksiyon kullanacağız):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = 2 * x ** 4 + y ** 2 - 4 * x\n",
    "\n",
    "x0, y0 = -1.4, 2\n",
    "\n",
    "for idx in range(3):\n",
    "\n",
    "    grad = np.array([\n",
    "        8 * x0 ** 3 - 4,\n",
    "        2 * y0,\n",
    "        0\n",
    "    ])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.contour(x, y, z, 50, cmap=\"viridis\")\n",
    "    plt.plot([x0], [y0], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "    plt.quiver(x0, y0, grad[0], grad[1], color=\"red\")\n",
    "    plt.title(f\"Iteration {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Noktaları gradyanın tersi yönünde ilerletelim.\n",
    "    x0 = x0 - grad[0]\n",
    "    y0 = y0 - grad[1]\n",
    "    \n",
    "    print(f\"X0: {x0}, Y0: {y0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilgisayarınızın ayarları ile oynamayın, böyle olması çok normaldi. Gradyanı direkt olarak kullanmak ile ilgili ciddi bir problem var, çünkü gradyan bize sadece belli bir yön sağlamıyor, aynı zamanda uzunluğu bize artığın miktarını veriyor,\n",
    "yani çok dik bir noktadaysak vektörümüzün uzunluğu çok büyük olacak ve bizi istediğimizden çok çok daha fazla hareket ettirecek, bu yüzden __learning rate__ dediğimiz kavramı kullanıyoruz, bu tamamen gradyan vektörümüzün bu kadar büyük adımlar\n",
    "atmasını istemediğimiz ve adım adım ilerlemek istediğimiz için, gelin bir de bu şekilde deneyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "z = 2 * x ** 4 + y ** 2 - 4 * x\n",
    "\n",
    "x0, y0 = -1.4, 2\n",
    "\n",
    "for idx in range(10):\n",
    "\n",
    "    grad = np.array([\n",
    "        8 * x0 ** 3 - 4,\n",
    "        2 * y0,\n",
    "        0\n",
    "    ])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.contour(x, y, z, 50, cmap=\"viridis\")\n",
    "    plt.plot([x0], [y0], \"ro\", markersize=10, alpha=0.8, zorder=10)\n",
    "    plt.quiver(x0, y0, grad[0], grad[1], color=\"red\")\n",
    "    plt.title(f\"Iteration {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Noktaları gradyanın tersi yönünde ilerletelim.\n",
    "    x0 = x0 - grad[0] * 0.1\n",
    "    y0 = y0 - grad[1] * 0.1\n",
    "    \n",
    "    print(f\"X0: {x0}, Y0: {y0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu şekilde yaptığımızda gördüğünüz gibi çok daha iyi çalışıyor, burada __learning rate__'i seçmek çoğunlukla deneme yanılmaya kalıyor, genelde belli problemler için ve belli modeller için\n",
    "bu değeri ne seçeceğimiz aşağı yukarı bellidir, ama yine de kesin bir tespit metodundan bahsetmek çok da mümkün değil.\n",
    "\n",
    "Tabii bu doğru adımlar atma problemi sadece learning rate'i ayarlamak ile çözülmüyor, bu problemi çok daha karmaşık problemlerde çok daha iyi çözmek için gradient descent'in farklı alternatifleri\n",
    "mevcut, ama bu alternatiflere burada girmeyeceğiz, notebook'un sonunda bunlara bakmanız için sizlere birkaç link vereceğim.\n",
    "\n",
    "Şimdi asıl olan olaya gelelim ve görselleştirmeleri bırakalım. Gradient Descent'i matematiksel olarak ifade etmemiz gerekirse, her bir iterasyonu şu şekilde tanımlayabiliriz:\n",
    "\n",
    "$ x_{n+1} = x_n - \\alpha \\nabla f(x_n) $\n",
    "\n",
    "Burada $x_n$ bizim bulunduğumuz nokta, $x_{n+1}$ ise bir sonraki iterasyonda bulunacağımız nokta, $\\alpha$ ise learning rate'imiz, $\\nabla f(x_n)$ ise gradyanımız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi geçen sefer kullandığımız __closed form__ least squares method fonksiyonlarını geri getirelim ve bunlardan aldığımız sonuç ile Gradient Descent'in sonucunu karşılaştıralım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(w: np.ndarray, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    \"\"\" Bu fonksiyon parametre olarak aldığı w ağırlık vektörü için\n",
    "    ortalama hata kareler toplamını (mean squared error) hesaplayıp geri döndürsün.\n",
    "\n",
    "    Önemli not 1: For veya while döngüsü kullanmadan, sadece matris işlemleri ile bu fonksiyonu yazın.\n",
    "\n",
    "    Args:\n",
    "        w (numpy.ndarray): Ağırlık vektörü. Boyutu D x 1.\n",
    "\n",
    "    Returns:\n",
    "        float: Hata değeri.\n",
    "    \"\"\"\n",
    "\n",
    "    err = 0.0 # Hesaplandıktan sonra döndürülecek hata değeri\n",
    "\n",
    "    # TODO: Hata değerini hesaplayın ve err değişkenine atayın.\n",
    "\n",
    "    diff = Y - X @ w\n",
    "    err = np.sum(diff ** 2)\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "def find_best_w(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Bu fonksiyon parametre olarak aldığı X ve Y matrislerini kullanarak\n",
    "    en iyi w değerini bulup geri döndürür.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): X matrisi. Boyutu N x D.\n",
    "        Y (numpy.ndarray): Y matrisi. Boyutu N x 1.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: En iyi w değeri. Boyutu D x 1.\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros((X.shape[1], 1)) # En başta w değerimizi 0'lar ile başlatalım.\n",
    "\n",
    "    # TODO: w değerini bulun ve w değişkenine atayın.\n",
    "\n",
    "    w = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burada tamamen yapay çok boyutlu bir veri seti oluşturuyoruz.\n",
    "\n",
    "# Tamamen rastgele bir X matrisi oluşturalım.\n",
    "X = np.random.randn(10000, 10)\n",
    "\n",
    "# W değerlerimizi de rastgele oluşturalım.\n",
    "W = np.random.randn(10, 1)\n",
    "\n",
    "# Y değerlerimizi X matrisi ile W matrisinin çarpımı olarak oluşturalım.\n",
    "Y = X @ W\n",
    "\n",
    "# Ufak bir parça gürültü ekleyelim.\n",
    "Y += np.random.randn(10000, 1) * 0.1\n",
    "\n",
    "# Burada sadece X ve Y matrislerini kullanarak az önce yarattığımız W değerini bulmaya çalışacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form çözüm\n",
    "\n",
    "w_best = find_best_w(X, Y)\n",
    "\n",
    "print(f\"En iyi w değeri: {w_best}\")\n",
    "\n",
    "print(f\"Gerçek w değeri: {W}\")\n",
    "\n",
    "hata = E(w_best, X, Y)\n",
    "\n",
    "print(f\"En iyi w değeri ile elde edilen ortalama hata kareler toplamı: {hata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gördüğünüz gibi oldukça iyi çalışıyor, peki ya Gradient Descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_w_gradient_descent(X, Y, learning_rate=0.0001, num_iterations=10000):\n",
    "    \"\"\" Bu fonksiyon parametre olarak aldığı X ve Y matrislerini kullanarak\n",
    "    en iyi w değerini bulup geri döndürür.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): X matrisi. Boyutu N x D.\n",
    "        Y (numpy.ndarray): Y matrisi. Boyutu N x 1.\n",
    "        learning_rate (float, optional): Öğrenme oranı. Defaults to 0.0001.\n",
    "        num_iterations (int, optional): İterasyon sayısı. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: En iyi w değeri. Boyutu D x 1.\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = X.shape # N ve D değerlerini alalım\n",
    "\n",
    "    # En başta w değerimizi rastgele ufak değerler ile başlatalım.\n",
    "    w = np.random.randn(D, 1) * 0.00001\n",
    "\n",
    "    # Şimdi de istediğimiz sayıda iterasyon yapalım\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Hatanın w'ya göre gradyanını hesaplayalım (Önceki örnekte bunu kağıt üstünde bulduktan sonra 0'a eşitliyorduk)\n",
    "        # Bu sefer diretk olarak gradyanın kendisini hesaplayacağız\n",
    "        grad = ...\n",
    "\n",
    "        # Burada ufak bir değişikliğe gitmemiz gerekiyor, önceden hatayı direkt olarak kullanıyorduk, toplam hata örnek\n",
    "        # sayımız arttıkça büyüyeceği için hatayı örnek sayısına bölerek ortalama bir hata bulduğumuzu düşünelim\n",
    "        # Bu durumda basit bir sabit ile çarpma işlemi yaptığımız için hem sonuç olarak çıkan gradyanı hem de hata\n",
    "        # değerini direkt olarak N'e bölebiliriz\n",
    "\n",
    "        grad = grad / N\n",
    "\n",
    "        # W değerini güncelleyelim (Gradient Descent)\n",
    "        w = ...\n",
    "\n",
    "        # Her 1000 iterasyonda bir hatayı ekrana yazdıralım\n",
    "        if (i+1) % 5000 == 0:\n",
    "            hata = E(w, X, Y) / N\n",
    "            print(f\"{i+1}. iterasyon, ortalama hata kareler toplamı: {hata}\")\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent ile w değerini bulalım\n",
    "\n",
    "# 1000 ile 10000 iterasyon arasında bir değer seçebilirsiniz.i\n",
    "# Learning rate 1.e-3 ile 1.e-5 arasında iyi çalışacaktır.\n",
    "w_best = ...\n",
    "\n",
    "print(f\"En iyi w değeri: {w_best}\")\n",
    "\n",
    "print(f\"Gerçek w değeri: {W}\")\n",
    "\n",
    "hata = E(w_best, X, Y)\n",
    "\n",
    "print(f\"En iyi w değeri ile elde edilen ortalama hata kareler toplamı: {hata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğer 100'e yakın bir hata ve orijinal $w$'ya yakın bir $w$ değeri elde ettiyseniz, tebrikler, Gradient Descent'i başarıyla uyguladınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
