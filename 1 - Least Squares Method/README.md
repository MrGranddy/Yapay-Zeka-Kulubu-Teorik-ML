# Least Squares Method

## Ã–nsÃ¶z

Merhaba arkadaÅŸlar, girizgah Ã¶nemli olduÄŸu iÃ§in bu haftaki materyali kendim hazÄ±rlamaya karar verdim, bu sayede eÄŸitim grubunun devamÄ±nda karÅŸÄ±mÄ±za Ã§Ä±kacak kavramlarÄ± size daha iyi aktarabileceÄŸimi dÃ¼ÅŸÃ¼ndÃ¼m.

Materyalin genelinde $\mathcal{D} = \{x^{(i)}\}_1^N, x^{(i)} \in \mathbb{R}^2$ Ã¶rneÄŸi gibi **matematiksel notasyon** kullanÄ±mÄ± ile sÄ±k sÄ±k karÅŸÄ±laÅŸacaksÄ±nÄ±z, her ne kadar basit kavramlarÄ± anlatÄ±rken pek ihtiyacÄ±mÄ±z olmasa da gÃ¼nÃ¼n sonunda matematiksel notasyon okur yazarlÄ±ÄŸÄ±mÄ±z olmasÄ± Ã¶nemli, bir noktadan sonra kavramlarÄ± baÅŸka ÅŸekilde anlatmanÄ±n pratik bir yolu kalmÄ±yor. Bu yazÄ±da aynÄ± zamanda size sÄ±kÃ§a kullanacaÄŸÄ±mÄ±z bazÄ± notasyonlarÄ± da gÃ¶stermeye Ã§alÄ±ÅŸacaÄŸÄ±m, sizden ricam onlarÄ± da asÄ±l materyal gibi Ã¶nemseyip kafanÄ±zda oturtmaya Ã§alÄ±ÅŸmanÄ±z.

## Nedir?

**Least Squares Method**, genel olarak yapay zeka temalÄ± birÃ§ok dersin olmazsa olmazlarÄ±ndan olan bir yÃ¶ntem. Ä°leride karÅŸÄ±mÄ±za Ã§Ä±kacak birÃ§ok kavramÄ± basit bir ÅŸekilde kullanmamÄ±z aÃ§Ä±sÄ±ndan gÃ¼zel bir Ã¶rnek.

Elinizde $D$ boyutlu $N$ adet vektÃ¶r olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼n. Ã–rneÄŸin $D = 2$ ve $N = 100$ ise bu elimizde $(x, y)$ gibi gÃ¶sterebileceÄŸimiz $100$ adet nokta var demek. Burada Ã¶nemli bir detay, Ã§oÄŸu zaman **vektÃ¶r** ve **nokta** terimleri birbirinin yerine kullanÄ±labiliyor, yani basitÃ§e elimizde hepimizin bildiÄŸi 2 boyutlu bir koordinat sistemindeki noktalar var diyebiliriz.

![Least Squares Method Ã–rnek Data](lsm_example_data.png)

Bunu gÃ¶stermenin alternatif bir yolu da elimizde $\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}^2$ olmak Ã¼zere bir veri kÃ¼mesi (**dataset**) olduÄŸunu sÃ¶ylemek. $D$ ve $N$'i genel hali ile bÄ±rakmak istersek ise

$\mathcal{D} = \{x^{(i)}\}_{i=1}^{N}, x^{(i)} \in \mathbb{R}^D$ diyebiliriz.

Buraya kadar her ÅŸey gÃ¼zel, peki bu tÃ¼r bir veri kÃ¼mesi ile karÅŸÄ±laÅŸabileceÄŸimiz gerÃ§ek bir senaryo ne olabilir? Yine 2 boyutlu bir Ã¶rnek Ã¼stÃ¼nden gidelim, Ã¶rneÄŸin Ä°stanbul'daki belli sayÄ±da arsa iÃ§in elimizde arsanÄ±n alanÄ± ve satÄ±ÅŸ fiyatÄ± olsun.

![Least Squares Method Ã–rnek Data](lsm_arsa_ornegi.png)

SayÄ±lara takÄ±lmamaya Ã§alÄ±ÅŸÄ±n :) Peki, elimizde bu data mevcut ve elimize yeni bir arsa geldiÄŸi zaman fiyatÄ±nÄ± tahmin etmek istiyoruz, bu durumda ne yapabiliriz? Ä°ÅŸte burada **Least Squares Method** devreye giriyor.

![Least Squares Method Ã–rnek Data](lsm_arsa_ornegi_with_line.png)

EÄŸer elimizde kÄ±rmÄ±zÄ± ile gÃ¶sterilen doÄŸrunun denklemi varsa, yeni bir arsa iÃ§in sadece arsanÄ±n alanÄ±nÄ± bilerek bir fiyat tahmini yapabiliriz. Least Squares Method bizim bu doÄŸruyu bulmamÄ±zÄ± saÄŸlÄ±yor.

Formal bir tanÄ±m yapacak olursak: Elimizde $\mathcal{D} = \{x^{(i)}\}_{i=1}^{N}, x^{(i)} \in \mathbb{R}^{D+1}$

veri kÃ¼mesi olsun, yani elimizdeki her bir vektÃ¶r $D+1$ boyutlu, Ã¶rneÄŸin vektÃ¶rlerimizin her bir elemanÄ± **alan**, **fiyat**, **en**, **boy** gibi Ã¶zellikleri tanÄ±mlÄ±yor olabilir, $x^{(i)} \in \mathbb{R}^{D+1}$ dediÄŸimizde ise bu elemanlarÄ±n birer reel sayÄ± olduÄŸunu ifade ediyor.

Daha aÃ§Ä±k sÃ¶ylemek gerekirse $x^{(1)}$, $x^{(2)}$, ..., $x^{(N)}$ gibi isimlendirilen, elimizdeki her bir vektÃ¶r $D+1$ adet reel sayÄ± ile ifade ediliyor. Az Ã¶nce $D$ derken ÅŸimdi $D+1$ demem kafanÄ±zÄ± karÄ±ÅŸtÄ±rmasÄ±n, bir sonraki adÄ±mÄ± daha anlaÅŸÄ±lÄ±r kÄ±lmak iÃ§in yaptÄ±ÄŸÄ±m bir deÄŸiÅŸiklik.

Åimdi her bir vektÃ¶r iÃ§in bu $D+1$ adet reel sayÄ±dan bir tanesini kenara ayÄ±ralÄ±m (Ã¶rneÄŸin arsanÄ±n fiyatÄ±) ve bunlardan yeni bir kÃ¼me oluÅŸturalÄ±m:

$\mathcal{Y} = \{y^{(i)}\}_{i=1}^{N}, y^{(i)} \in \mathbb{R}$

Her bir vektÃ¶rden sadece tek bir elemanÄ± ayÄ±rdÄ±ÄŸÄ±mÄ±z iÃ§in yeni kÃ¼memizdeki elemanlar da sadece tek boyutlu, bu yÃ¼zden $y^{(i)} \in \mathbb{R}$ diyebiliyoruz. Ã–rneÄŸin Ã¶nceden elimizde (**alan**, **fiyat**, **en**, **boy**) ÅŸeklinde vektÃ¶rler varken, ÅŸimdi elimizde (**alan**, **en**, **boy**) ve ( **fiyat** ) olmak Ã¼zere iki ayrÄ± Ã§eÅŸit vektÃ¶r var.

ArtÄ±k elimizde iki adet kÃ¼me var:

$\mathcal{D} = \{x^{(i)}\}_{i=1}^{N}, x^{(i)} \in \mathbb{R}^{D}$ ve

$\mathcal{Y} = \{y^{(i)}\}_{i=1}^{N}, y^{(i)} \in \mathbb{R}$

Ä°ÅŸte bu yÃ¼zden $D$ yerine $D+1$ demiÅŸtim :) KarÄ±ÅŸÄ±k gÃ¶rÃ¼nÃ¼yor olabilir ama aslÄ±nda Ã§ok basit, elimizdeki veri kÃ¼mesini iki parÃ§aya ayÄ±rdÄ±k, bir tanesi Ã¶zellikleri iÃ§eriyor, diÄŸeri ise fiyatlarÄ± iÃ§eriyor ve elbette $x^{(i)}$ Ã¶zelliklerine karÅŸÄ±lÄ±k gelen $y^{(i)}$ fiyatÄ± ile eÅŸleÅŸiyor, yani $x^{(1)}$ Ã¶zelliklerine sahip arsanÄ±n fiyatÄ± $y^{(1)}$, $x^{(2)}$ Ã¶zelliklerine sahip arsanÄ±n fiyatÄ± $y^{(2)}$ ve bÃ¶yle devam ediyor.

DeÄŸiÅŸken isimleri ve bir tÄ±k karmaÅŸÄ±k bazÄ± matematiksel zamazingolar ile kafanÄ±zÄ± ÅŸiÅŸirdiÄŸim iÃ§in kusuruma bakmayÄ±n :) Ama inanÄ±n her ÅŸey sizin iyiliÄŸiniz iÃ§in.

Åimdi her ÅŸeyin yerli yerine oturmasÄ± iÃ§in asÄ±l Ã¶rneÄŸimizde bu yeni kÃ¼meleri tanÄ±mlayalÄ±m. Ã–ncelikle elimizdeki veri kÃ¼mesi:

![Least Squares Method Ã–rnek Data](lsm_arsa_ornegi.png)

Bu veri kÃ¼mesini $\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}^2$ ÅŸeklinde tanÄ±mlamÄ±ÅŸtÄ±k, az Ã¶nce yaptÄ±ÄŸÄ±mÄ±z gibi iki parÃ§aya ayÄ±rdÄ±ÄŸÄ±mÄ±zda ise elimizde:

$\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}$ ve

$\mathcal{Y} = \{y^{(i)}\}_{i=1}^{100}, y^{(i)} \in \mathbb{R}$

ÅŸeklinde iki kÃ¼me oluyor. Yani bir kÃ¼me sadece arsa alanlarÄ±nÄ± iÃ§erirken, diÄŸeri ise sadece fiyatlarÄ± iÃ§eriyor. ArtÄ±k problemimizi daha kolay bir ÅŸekilde ifade edebiliriz, herhangi bir $x^{(i)}$ iÃ§in $y^{(i)}$'yi tahmin etmek istiyoruz, yani herhangi bir arsanÄ±n alanÄ± bilindiÄŸinde fiyatÄ±nÄ± tahmin etmek istiyoruz. Tabii ki $x^{(i)}$ 1 boyutlu olmak zorunda deÄŸildi, alan bilgisinin yanÄ±nda daha bir sÃ¼rÃ¼ Ã¶zelliÄŸi de iÃ§inde barÄ±ndÄ±rabilirdi, bu yÃ¼zden genel senaryo iÃ§in $x^{(i)} \in \mathbb{R}^D$ diyoruz. Ama eÄŸer bu Ã¶rnekte $D$'yi 1'den bÃ¼yÃ¼k bir sayÄ± belirleseydik, gÃ¶rselleÅŸtirmemiz oldukÃ§a zorlaÅŸacaktÄ±, hadi 2 olsa yine bir ÅŸekilde yapardÄ±k ama 3'ten sonrasÄ±nÄ± gÃ¶rsel olarak kafasÄ±nda canlandÄ±ran varsa helal olsun :) (UnutmayÄ±n $x$'in 3 elemanÄ± yanÄ±nda bir de karÅŸÄ±lÄ±k olarak $y$'nin 1 elemanÄ± var, yani 4 boyutlu bir uzaydan bahsediyoruz)

Åimdi elle tutulur ÅŸeylere geri dÃ¶nelim, ne demiÅŸtik:

![Least Squares Method Ã–rnek Data](lsm_arsa_ornegi_with_line.png)

KÄ±rmÄ±zÄ± doÄŸruyu Ã§izersek, problemi Ã§Ã¶zeriz. KaÄŸÄ±t kalem ile yapmasÄ± oldukÃ§a kolay, ama bahsettiÄŸimiz $D$'nin $1$ olmadÄ±ÄŸÄ± senaryolar iÃ§in biraz daha matematiksel dÃ¼ÅŸÃ¼nelim. En nihayetinde elde etmek istediÄŸimiz ÅŸey, herhangi bir arsa alanÄ± iÃ§in bu arsanÄ±n fiyatÄ±nÄ± kusursuz bir ÅŸekilde tahmin etmek, elbette arsanÄ±n sadece alanÄ±nÄ± biliyorsak bu oldukÃ§a zor bir iÅŸ, hatta daha fazla bilgimiz olsa bile, hayatta her ÅŸey rasyonel deÄŸil, birileri arsasÄ±nÄ± Ã§ok uzuca veya Ã§ok pahalÄ±ya satÄ±yor olabilir, gerÃ§ek hayatta karÅŸÄ±mÄ±za Ã§Ä±kan bu tÃ¼r keyfi oynamalara __noise__ yani __gÃ¼rÃ¼ltÃ¼__ deriz. Yani kusursuz tahmin iÃ§in Ã§ok da heveslenmemekte fayda var, ama biz yine de elimizden geleni yapalÄ±m.

Daha da basitleÅŸtirmek istersek, elimizde $1$ adet reel sayÄ± var ve bir buna karÅŸÄ±lÄ±k baÅŸka $1$ reel sayÄ±yÄ± tahmin etmek istiyoruz. Bu bize Ã§ok iyi bildiÄŸimiz bir yapÄ±yÄ± anÄ±msatÄ±yor: __fonksiyonlar__.

Ä°deal bir dÃ¼nyada sihirli bir ÅŸekilde Ã¶yle bir $f(x)$ fonksiyonu elde edebiliriz ki, $f(x^{(1)}) = y^{(1)}$ olur, $f(x^{(2)}) = y^{(2)}$ olur, $f(x^{(3)}) = y^{(3)}$ olur ve bÃ¶yle devam eder. Elimizde bu fonksiyon varsa, fiyatÄ±nÄ± bilmeyip alanÄ±nÄ± bildiÄŸimiz herhangi bir arsa iÃ§in, bu fonksiyona alanÄ± verdiÄŸimizde fiyatÄ±nÄ± tahmin edebiliriz. Peki bu fonksiyonu nasÄ±l elde edeceÄŸiz?

Yine bildiÄŸimiz basit gerÃ§eklere dÃ¶nelim, kÄ±rmÄ±zÄ± doÄŸruyu hatÄ±rlayÄ±n:

![Least Squares Method Ã–rnek Data](lsm_arsa_ornegi_with_line.png)

2 boyutlu bu doÄŸrunu denkleminin neye benzeyeceÄŸini Ã§ok iyi biliyoruz: $f(x) = a \times x + b$

Yani aslÄ±nda bulmamÄ±z gereken $2$ adet deÄŸiÅŸken var, $a$ ve $b$. Ä°nsan dili ile ifade etmemiz gerekirse eÄŸim ve y eksenini kestiÄŸi nokta. Åimdi gelin makine Ã¶ÄŸrenmesindeki en temel kavramlardan bir tanesini tanÄ±yalÄ±m ve Ã§Ã¶zÃ¼me bir adÄ±m daha yaklaÅŸalÄ±m: __Error Function__ yani __hata fonksiyonu__. 

Makine Ã¶ÄŸrenmesinde uyguladÄ±ÄŸÄ±mÄ±z en temel stratejilerden biri, Ã§ok kÃ¶tÃ¼ bir tahmin fonksiyonunu alÄ±p adÄ±m adÄ±m iyileÅŸtirmek. Ne mi demek istiyorum? Diyelim elimizde arsa fiyatÄ± tahmini iÃ§in bir fonksiyon var ve adÄ± $g(x)$. 

Peki elimizde $g(x)$ var, ve gerÃ§ekten de ona bir arsanÄ±n alanÄ±nÄ± sÃ¶ylediÄŸimizde bize fiyatla ilgili bir tahmin yapÄ±yor. Peki $g(x)$'in genel olarak ne kadar baÅŸarÄ±lÄ± olduÄŸunu nasÄ±l anlayacaÄŸÄ±z? Evet elimizde az sayÄ±da Ã¶rnek varsa tek tek hesaplayÄ±p bakabiliriz ama tahmin edeceÄŸiniz gibi bu pek de iyi bir fikir deÄŸil.

Åu iÅŸimizi Ã§ok daha kolaylaÅŸtÄ±rÄ±rdÄ±, Ã¶yle bir fonksiyon var ki, ben elimdeki $g(x)$ fonksiyonunun yaptÄ±ÄŸÄ± tahminleri ona verince bana pozitif bir reel sayÄ± dÃ¶ndÃ¼rÃ¼yor, bu sayÄ± da $g(x)$'in ne kadar baÅŸarÄ±lÄ± olduÄŸunu gÃ¶steriyor. Bu sayÄ± ne kadar kÃ¼Ã§Ã¼kse, $g(x)$ o kadar baÅŸarÄ±lÄ± demek. Bu sayÄ±ya __error__ (__hata__) diyoruz, yani $g(x)$'in yaptÄ±ÄŸÄ± tahminlerin ne kadar yanlÄ±ÅŸ olduÄŸunu gÃ¶steren bir sayÄ±.

Diyelim ki $\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}$ veri kÃ¼mesindeki tÃ¼m $x^{(i)}$'leri $g(x)$'e veriyoruz ve hepsi iÃ§in bir tahmin alÄ±yoruz, bu tahminleri $\hat{y}^{(i)}$ ÅŸeklinde isimlendirelim. Yani $g(x^{(i)}) = \hat{y}^{(i)}$. Dikkat edin $\hat{y}$'nin Ã¼zerinde bir ÅŸapka var.

Åimdi asÄ±l noktaya geliyoruz, $\hat{y}^{(i)}$ bizim tahminimiz, $y^{(i)}$ ise gerÃ§ek deÄŸer, tÃ¼m $i$'ler iÃ§in bu iki deÄŸer eÅŸit olsaydÄ±, bu $g(x)$'in kusursuz Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterirdi, yani bu __hata__ dediÄŸimiz ÅŸeyi hesaplamanÄ±n bir yolunu bilseydik, hatanÄ±n $0$ olmasÄ±nÄ± beklerdik.

O zaman ÅŸÃ¶yle bir ÅŸey yapalÄ±m, her bir $i$ iÃ§in $y^{(i)} - \hat{y}^{(i)}$ hesaplayalÄ±m ve bunlarÄ± toplayalÄ±m, $g(x)$'in kusursuz Ã§alÄ±ÅŸtÄ±ÄŸÄ± senaryoda bu deÄŸer gerÃ§ekten de 0 gelirdi. Ama burada bir ÅŸey ters, deÄŸerlerin eÅŸit olmadÄ±ÄŸÄ± senaryoda $\hat{y}^{(i)}$ bÃ¼yÃ¼dÃ¼kÃ§e toplam deÄŸer kÃ¼Ã§Ã¼lÃ¼r, yani hata azalÄ±r, ama durum bu deÄŸil, Ã§Ã¼nkÃ¼ $\hat{y}^{(i)}$ bÃ¼yÃ¼yorsa $y^{(i)}$ ile arasÄ±ndaki fark aÃ§Ä±lÄ±yor demek! Bu yÃ¼zden $y^{(i)} - \hat{y}^{(i)}$'nin mutlak deÄŸerini alÄ±p toplamak daha mantÄ±klÄ±, yani $|y^{(i)} - \hat{y}^{(i)}|$, iÅŸte bu bize gerÃ§ekten de tahmin edilen fiyatlar ve gerÃ§ek fiyatlarÄ±n birbirine ne kadar benzediÄŸini gÃ¶sterir. Benzerlerse deÄŸer kÃ¼Ã§Ã¼k olur, deÄŸillerse deÄŸer bÃ¼yÃ¼k olur. Ve fark edeceÄŸiniz Ã¼zere bizim eÅŸde etmek istediÄŸimiz __hata fonksiyonu__'nun tam olarak bunu yapmasÄ± gerekiyor! O zaman fonksiyonumuzu bulduk.

$E = âˆ‘_{i=1}^{100} |y^{(i)} - \hat{y}^{(i)}|$

Burada $E$ __error__'Ä±n baÅŸ harfinden geliyor. Peki ÅŸimdi hatÄ±rlayÄ±n $g(x) = a \times x + b$ formunda demiÅŸtik, yani her farklÄ± $a$ ve $b$ deÄŸeri iÃ§in $g(x)$ farklÄ± bir doÄŸruyu ifade ediyor. Biz bu farklÄ± doÄŸrular arasÄ±nda bize en dÃ¼ÅŸÃ¼k hatayÄ± veren $g(x)$'i istiyoruz. O zaman hata fonksiyonunu $a$ ve $b$ cinsinden yazabiliriz:

$E(a, b) = âˆ‘_{i=1}^{100} |y^{(i)} - (a \times x^{(i)} + b)|$

Burada $E$'nin yanÄ±na $a$ ve $b$ yazdÄ±k, Ã§Ã¼nkÃ¼ $E$'nin $a$ ve $b$'ye baÄŸlÄ± olduÄŸunu belirtmek istiyoruz, en nihayetinde datamÄ±z yani arsalara karÅŸÄ±lÄ±k gelen alan ve fiyat ikilileri deÄŸiÅŸmiyor, deÄŸiÅŸen ÅŸey $a$ ve $b$ yani datamÄ±za uygun olacak ÅŸekilde deÄŸiÅŸtireceÄŸimiz deÄŸiÅŸkenler.

Bu noktada ÅŸunu belirtmekte fayda var, makine Ã¶ÄŸrenmesi gibi matematik temeli yoÄŸun alanlarda, en basit yÃ¶ntem iÃ§in bile tÃ¼m iÅŸlemleri, denklemleri, deÄŸiÅŸkenleri sÃ¼rekli aklÄ±mÄ±zda tutmamÄ±z Ã§ok mÃ¼mkÃ¼n olmayabilir. O yÃ¼zden bu noktada baÅŸtan sona tÃ¼m aÅŸamalar aklÄ±nÄ±zda deÄŸilse endiÅŸe etmeyin. Ã–nemli olan aÅŸamalar arasÄ± her bir ufak geÃ§iÅŸi anlamak, eÄŸer ÅŸu noktaya kadar her bir adÄ±mÄ± anladÄ±ysanÄ±z, baÅŸta elimizde olan ÅŸey ile ÅŸu an elimizde olan ÅŸey arasÄ±nda direkt bir baÄŸlantÄ± gÃ¶rmenize gerek yok, en nihayetinde her bir adÄ±mdan tek tek eminseniz, ÅŸu an elinizdeki ÅŸey doÄŸru demektir.

Peki dediÄŸimiz gibi burada verimiz sabit, yani elimizde aynÄ± 100 adet arsa, arsalarÄ±n alanlarÄ± ve fiyatlarÄ± var, yani aslÄ±nda __hata fonksiyonu__'muz iÃ§in $x^{(i)}$ ve $y^{(i)}$ sabit. __Hata fonksiyonu__ sadece ve sadece $a$ ve $b$ deÄŸiÅŸkenlerine baÄŸlÄ±!

Bu iÅŸimizi Ã§ok ama Ã§ok kolaylaÅŸtÄ±rÄ±yor. Elimizde bir $E(a, b)$ fonksiyonu var ve bu fonksiyonun deÄŸerini olabilecek en kÃ¼Ã§Ã¼k deÄŸer yapan $a$ ve $b$ deÄŸiÅŸkenlerini bulmak istiyoruz. Ne yapmamÄ±z gerektiÄŸini biliyorsunuz :)

EÄŸer $E(a, b)$'nin $a$ ve $b$'ye gÃ¶re tÃ¼revini alÄ±p 0'a eÅŸitleyip $a$ ve $b$ iÃ§in denklemi Ã§Ã¶zersek, $E(a, b)$'nin $a$ ve $b$'ye gÃ¶re minimum olduÄŸu noktayÄ± bulmuÅŸ oluruz. KÄ±saca hatÄ±rlayalÄ±m:

$h(a, b) = 5a^2 + 3ab + 2b^2 + 3a + 5b + 1$ olsun. $h(a, b)$'nin $a$'ya gÃ¶re tÃ¼revini alalÄ±m:

$\frac{\partial h(a, b)}{\partial a} = 10a + 3b + 3$ ÅŸimdi Ã§Ä±kan ifadeyi $0$'a eÅŸitleyelim:

$10a + 3b + 3 = 0$ buradan $a = -\frac{3b}{10} - \frac{3}{10}$ elde ederiz. Åimdi $h(a, b)$'nin $b$'ye gÃ¶re tÃ¼revini alalÄ±m:

$\frac{\partial h(a, b)}{\partial b} = 3a + 4b + 5$ ÅŸimdi Ã§Ä±kan ifadeyi $0$'a eÅŸitleyelim:

$3a + 4b + 5 = 0$ buradan $b = -\frac{3a}{4} - \frac{5}{4}$ elde ederiz. Åimdi sistemimizi Ã§Ã¶zelim:

$a = -\frac{3b}{10} - \frac{3}{10}$

$b = -\frac{3a}{4} - \frac{5}{4}$

Maalesef bunu adÄ±m adÄ±m yapmayacaÄŸÄ±m Ã§Ã¼nkÃ¼ deÄŸerler Ã§ok saÃ§ma geliyor :) Ama 2 bilinmeyenli 2 denklemi Ã§Ã¶zmek iÃ§in bildiÄŸimiz yÃ¶ntemleri kullanabiliriz, sonuÃ§ ise:

$a \approx 0.0968$ ve $b \approx -1.3226$ olacak.

Tamamen aynÄ± mantÄ±k $E(a,b)$ fonksiyonumuz iÃ§in de geÃ§erli. Ama yine Ã¶nÃ¼mÃ¼zde ufak bir engel var. Pek tabii ki bunu yapmanÄ±n yollarÄ± var ama $E(a, b)$'yi mutlak deÄŸerli bir fonksiyon olarak tanÄ±mladÄ±k ve bu minimumunu bulmak iÃ§in Ã§ok gÃ¼zel bir fonksiyon deÄŸil. Ä°ÅŸte bu noktada __Least Squares Method__'un adÄ±nda da olan ufak bir deÄŸiÅŸiklik ile iÅŸimizi kolaylaÅŸtÄ±rabiliriz __squares__ yani __kareler__.

HatÄ±rlarsanÄ±z mutlak deÄŸeri kullanma sebebimiz rastgele farklara bakmaktansa iki deÄŸerin birbirine ne kadar uzak olduÄŸunu Ã¶lÃ§mekti, bunu yapmak iÃ§in illa mutlak deÄŸer kullanmak zorunda deÄŸiliz, ÅŸunu dÃ¼ÅŸÃ¼nÃ¼n $(y^{(i)} - \hat{y}^{(i)})^2$ deseydik, yine istediÄŸimiz Ã¶zellikleri saÄŸlamaz mÄ±ydÄ±? Evet saÄŸlardÄ±, Ã§Ã¼nkÃ¼ $(y^{(i)} - \hat{y}^{(i)})^2$'nin deÄŸeri her zaman pozitif ve deÄŸerler birbirine ne kadar uzaksa o kadar bÃ¼yÃ¼k olur. Bu yÃ¼zden __Least Squares Method__'da __squares__ yani __kareler__ kullanÄ±lÄ±yor. O zaman __hata fonksiyonu__'muzu buna gÃ¶re gÃ¼ncelleyelim:

$E(a, b) = âˆ‘_{i=1}^{100} (y^{(i)} - (a \times x^{(i)} + b))^2$.

Peki, teoride bu ifadenin tÃ¼revini alabileceÄŸimizi biliyoruz, daha sonrasÄ±nda $a$'ya ve $b$'ye gÃ¶re tÃ¼revleri $0$'a eÅŸitlememiz gerektiÄŸini de biliyoruz. AslÄ±nda ÅŸu an elinize kaÄŸÄ±t kalemi alÄ±p $a$ ve $b$ iÃ§in $x^{(i)}$ ve $y^{(i)}$'e gÃ¶re birer denklem Ã§Ä±karabilirsiniz, fakat bu bizi Ã§ok uÄŸraÅŸtÄ±rÄ±r. Gelin bir adÄ±m geri gidelim:

$E(a, b) = âˆ‘_{i=1}^{100} (y^{(i)} - \hat{y}^{(i)})^2$ demiÅŸtik. Hadi bir adÄ±m daha geri gidip $100$ yerine $N$ yazalÄ±m:

$E(a, b) = âˆ‘_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$.

Elimizde oldukÃ§a yalÄ±n bir ifade var. Bunu daha da yalÄ±nlaÅŸtÄ±rmanÄ±n bir yolu olabilir mi? Mesela ÅŸu toplam sembolÃ¼nden bir kurtulsak? HafÄ±zanÄ±zÄ± tazelemek iÃ§in bir Ã¶rnek dÃ¼ÅŸÃ¼nelim, elimde bir vektÃ¶r olsun:

```math
\mathbf{v} = \begin{bmatrix} a \\ b \\ c \\ d \\ e \end{bmatrix}
```

Bu vektÃ¶rÃ¼ kendisi ile skaler Ã§arpÄ±m yaparsak ne olur?

$\mathbf{v} \cdot \mathbf{v} = \mathbf{v}^T\mathbf{v} = a^2 + b^2 + c^2 + d^2 + e^2$

Yani her bir elemanÄ±n karelerinin toplamÄ±. O halde benim elimde elemanlarÄ± $y^{(i)} - \hat{y}^{(i)}$ olan bir vektÃ¶r olsa:

```math
\mathbf{v} = \begin{bmatrix} y^{(1)} - \hat{y}^{(1)} \\ y^{(2)} - \hat{y}^{(2)} \\ \vdots \\ y^{(N)} - \hat{y}^{(N)} \end{bmatrix}
```
ve bu vektÃ¶rÃ¼ kendisi ile skaler Ã§arpsam:

$\mathbf{v} \cdot \mathbf{v} = \mathbf{v}^T\mathbf{v} = âˆ‘_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$ ğŸ¤¯ğŸ¤¯ğŸ¤¯ğŸ¤¯

Hatta ve hatta $Y$ ve $\hat{Y}$ olmak Ã¼zere iki vektÃ¶rÃ¼mÃ¼z olsa:

```math
Y = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)} \end{bmatrix}$ ve $\hat{Y} = \begin{bmatrix} \hat{y}^{(1)} \\ \hat{y}^{(2)} \\ \vdots \\ \hat{y}^{(N)} \end{bmatrix}
```

$\mathbf{v} = Y - \hat{Y}$ yani:

$\mathbf{v}^T\mathbf{v} = (Y - \hat{Y})^T(Y - \hat{Y}) = âˆ‘_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$ ğŸ¤¯ğŸ¤¯ğŸ¤¯ğŸ¤¯ğŸ¤¯ğŸ¤¯

Toparlarsak:

$E(a, b) = \mathbf{v}^T\mathbf{v} = (Y - \hat{Y})^T(Y - \hat{Y}) = âˆ‘_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$

Åimdi iÅŸleri bir adÄ±m daha ileriye taÅŸÄ±yalÄ±m, en baÅŸta hata fonksiyonumuzun $a$ ve $b$'ye baÄŸlÄ± olmasÄ±nÄ±n sebebi, $g(x) = a \times x + b$ olmasÄ±ydÄ±. Bu da en baÅŸa dÃ¶nersek $D$ yani vektÃ¶rlerimizin boyutu $1$ olduÄŸu iÃ§indi. Peki ya $D$'yi de tekrar eski haline getirseydik, mesela her $x^{(i)} \in \mathbb{R}^D$ yani $D$ boyutlu birer vektÃ¶r olsaydÄ±? O zaman genel senaryoda:

```math
x^{(i)} = \begin{bmatrix} x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_D^{(i)} \end{bmatrix}
```
olurdu.

Yani artÄ±k sadece arsanÄ±n alanÄ± deÄŸil, arsa ile ilgili Ã§eÅŸit Ã§eÅŸit bilgilere sahibiz. Peki aynÄ± ÅŸekilde tahmin fonksiyonumuzu da genelleyemez miyiz? $g(x) = a \times x + b$ yerine:

$g(x) = w_1 \times x_1 + w_2 \times x_2 + ... + w_D \times x_D + b$

desek, her bir deÄŸere karÅŸÄ±lÄ±k bir katsayÄ± olan aslÄ±nda oldukÃ§a basit bir fonksiyon taslaÄŸÄ±mÄ±z olurdu. Tek bir Ã§arpÄ±m yapmak yerine $D$ Ã§arpÄ±m yapÄ±p toplamaktan baÅŸka hiÃ§bir fark yok. $g(x) = a \times x + b$'de $x$'e baÄŸlÄ± olmayan $b$'yi de olduÄŸu gibi bÄ±raktÄ±k. Yine matris Ã§arpÄ±mÄ± notasyonumuzu kullanarak bunu da basitleÅŸtirebiliriz:

```math
\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_D \end{bmatrix}, \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_D \end{bmatrix}
```
 dersek:

$g(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b$ olur.

Åimdi bu fonksiyonu kullanarak $\hat{y}^{(i)}$'yi yazalÄ±m:

$\hat{y}^{(i)} = g(x^{(i)}) = \mathbf{w}^T\mathbf{x}^{(i)} + b$

Bu noktada $b$ bizler iÃ§in Ã¶nemli bir katkÄ±, fakat iÅŸlemleri biraz da olsa karÄ±ÅŸtÄ±racak, o yÃ¼zden ÅŸimdilik onu iÅŸin iÃ§inden Ã§Ä±kartalÄ±m, devamÄ±nda $b$'yi Ã§Ä±karmadan yapmayÄ± da size bÄ±rakÄ±yorum. Åimdi $\hat{y}^{(i)}$'yi yazalÄ±m:

$\hat{y}^{(i)} = \mathbf{w}^T\mathbf{x}^{(i)}$ ÅŸimdi $\hat{Y}$ vektÃ¶rÃ¼nÃ¼ de tekrar elden geÃ§irelim:

```math
\hat{Y} = \begin{bmatrix} \hat{y}^{(1)} \\ \hat{y}^{(2)} \\ \vdots \\ \hat{y}^{(N)} \end{bmatrix} = \begin{bmatrix} \mathbf{w}^T\mathbf{x}^{(1)} \\ \mathbf{w}^T\mathbf{x}^{(2)} \\ \vdots \\ \mathbf{w}^T\mathbf{x}^{(N)} \end{bmatrix}
```

Bu noktada aslÄ±nda her ÅŸeyi Ã§ok Ã§ok daha basitleÅŸtireceÄŸimiz bir adÄ±m var. Bu adÄ±m matris Ã§arpÄ±mÄ±na aÅŸina olanlarÄ±nÄ±z iÃ§in bariz olabilir, ama deÄŸilse de sorun deÄŸil, bir sonraki adÄ±m sizin iÃ§in aÃ§Ä±k deÄŸilse bunu Ã§Ä±karmayÄ± da size bir Ã¶dev olarak bÄ±rakÄ±yorum :)

Ã–yle bir $X$ matrisi tanÄ±mlayabiliriz ki:

```math
X = \begin{bmatrix} \mathbf{x}^{(1)} \\ \mathbf{x}^{(2)} \\ \vdots \\ \mathbf{x}^{(N)} \end{bmatrix}$ burada her bir $\mathbf{x}^{(i)}
```

birer satÄ±r vektÃ¶rÃ¼. Bu durumda $\hat{Y}$'yi ÅŸu ÅŸekilde yazabiliriz:

$\hat{Y} = X\mathbf{w}$

BÃ¼tÃ¼n bu zorlu uÄŸraÅŸlarÄ±n ve emeklerimizin karÅŸÄ±lÄ±ÄŸÄ±nda, genel $N$ ve $D$ iÃ§in, hayal etmeye bile cÃ¼ret edemeyeceÄŸimiz basitlikte bir ifade elde ettik, son olarak hata fonksiyonumuzu da bu ifadeyi kullanarak yazalÄ±m:

$E(\mathbf{w}) = (Y - \hat{Y})^T(Y - \hat{Y}) = (Y - X\mathbf{w})^T(Y - X\mathbf{w}) = âˆ‘_{i=1}^{N} (y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)})^2$

Fark ettiÄŸiniz gibi artÄ±k __hata fonksiyonu__ $a$ ve $b$'ye deÄŸil, $\mathbf{w}$'ye baÄŸlÄ±, Ã§Ã¼nkÃ¼ artÄ±k $g(x) = \mathbf{w}^T\mathbf{x}$ ve fonksiyonu deÄŸiÅŸtiren parametrelerimiz $\mathbf{w}$ vektÃ¶rÃ¼nde. Bununla da sÄ±nÄ±rlÄ± kalmayalÄ±m, $(Y - X\mathbf{w})^T(Y - X\mathbf{w})$ ifadesini aÃ§alÄ±m:

$E(\mathbf{w}) = (Y - X\mathbf{w})^T(Y - X\mathbf{w}) = Y^TY - Y^TX\mathbf{w} - \mathbf{w}^TX^TY + \mathbf{w}^TX^TX\mathbf{w}$

Burada kullandÄ±ÄŸÄ±mÄ±z kurallar:

1. $(AB)^T = B^TA^T$ hatta $(ABC)^T = C^TB^TA^T$ ÅŸeklinde genelleyebiliriz
2. $(A + B)^T = A^T + B^T$

Ä°ÅŸimizi bir tÄ±k daha kolaylaÅŸtÄ±rmak iÃ§in bir numara daha var, hatÄ±rlarsanÄ±z $E(\mathbf{w})$ bize bir __hata__ deÄŸeri dÃ¶ndÃ¼rÃ¼yor ve bu deÄŸer sadece skaler bir reel sayÄ±. Bu da demek oluyor ki $E(\mathbf{w})$'yi elde etmek iÃ§in topladÄ±ÄŸÄ±mÄ±z $4$ ayrÄ± terim de en nihayetinde birer skaler olmak zorunda. Skaler bir sayÄ± iÃ§in ÅŸaÅŸÄ±rtÄ±cÄ± olmayacak ÅŸekilde transpozu kendisine eÅŸittir, Ã¶rneÄŸin $5^T = 5$. O halde $E(\mathbf{w})$'yi oluÅŸturan her bir terimin transpozu kendisine eÅŸit, burada gÃ¶rmÃ¼ÅŸ olabileceÄŸiniz Ã¼zere $(\mathbf{w}^TX^TY)^T = Y^TX\mathbf{w}$ ve bu terimden zaten bir tane mevcut, o halde ikisini toplayÄ±p __hata fonksiyonu__'nu son haline getirebiliriz:

$E(\mathbf{w}) = (Y - X\mathbf{w})^T(Y - X\mathbf{w})$

$= Y^TY - Y^TX\mathbf{w} - \mathbf{w}^TX^TY + \mathbf{w}^TX^TX\mathbf{w}$

$= Y^TY - Y^TX\mathbf{w} - (\mathbf{w}^TX^TY)^T + \mathbf{w}^TX^TX\mathbf{w}$

$= Y^TY - Y^TX\mathbf{w} - Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w}$

$= Y^TY - 2Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w}$

Yani son olarak elimizdeki her ÅŸeyi Ã¶zetlemek gerekirse:

$E(\mathbf{w}) = Y^TY - 2Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w}$

ve eÄŸer $\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = 0$'Ä± Ã§Ã¶zersek. Buradan hatayÄ± minimum yapan $\mathbf{w}^*$'yi bulmuÅŸ oluruz. ÃœstÃ¼ndeki * (asterisk) de artÄ±k bunun herhangi bir $\mathbf{w}$ deÄŸil, hatayÄ± minimum yapan $\mathbf{w}$ olduÄŸunu belirtmek iÃ§in kullanÄ±lan bir iÅŸaret.

Elbette $\mathbf{w}^*$, $Y$ ve $X$'e baÄŸlÄ± bir deÄŸer Ã§Ä±kacak. Bu noktada iÅŸin asÄ±l keyifli tarafÄ± olan optimal 

$\mathbf{w}^*$'yi bulma iÅŸini size bÄ±rakÄ±yorum. Evet baÅŸka korkutucu gÃ¶zÃ¼kebilir ama bir kere baÅŸardÄ±ÄŸÄ±nÄ±zda baÄŸÄ±mlÄ±sÄ± olacaksÄ±nÄ±z. Tabii ki sizi bu konuda yalnÄ±z bÄ±rakmamak iÃ§in birkaÃ§ ipucu ve kaynak ile iÅŸi noktalÄ±yorum. Bu aÅŸamadan sonra ise belki daha da keyifli olacak olan ellerimizle hazÄ±rladÄ±ÄŸÄ±mÄ±z bu metodun kodunu yazacaÄŸÄ±z ve sonuÃ§larÄ± gÃ¶receÄŸiz, bunun detaylarÄ± iÃ§in Whatsapp grubunu takipte kalÄ±n :)

## Ä°puÃ§larÄ±

$\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}}$ demek, skaler deÄŸerli bir fonksiyonun bir vektÃ¶re gÃ¶re gradyanÄ± demek.

GradyanÄ±n ne olduÄŸu ve nasÄ±l alÄ±ndÄ±ÄŸÄ± ile ilgili kaynak: https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient

Fiziksel aÃ§Ä±klamalarÄ± geÃ§ip direkt olarak nasÄ±l hesaplandÄ±ÄŸÄ±na bakabilirsiniz, aslÄ±nda Ã§ok basit $\mathbf{w}$'deki her bir eleman iÃ§in, tek tek o elemana gÃ¶re tÃ¼m ifadenin tÃ¼revini alÄ±n ve bunlarÄ± bir vektÃ¶r olarak alt alta yazÄ±n, yani:

```math
\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \begin{bmatrix} \dfrac{\partial E(\mathbf{w})}{\partial w_1} \\ \dfrac{\partial E(\mathbf{w})}{\partial w_2} \\ \vdots \\ \dfrac{\partial E(\mathbf{w})}{\partial w_D} \end{bmatrix}
```

Tabii ki iÅŸ bunla bitmeyecek, gradyan da tÃ¼rev gibi toplamaya daÄŸÄ±labilen bir iÅŸlem o yÃ¼zden iÅŸi biraz basitleÅŸtirmek iÃ§in, ÅŸunu da size verebilirim:

$\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \dfrac{\partial (Y^TY - 2Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w})}{\partial \mathbf{w}} = \dfrac{\partial (Y^TY)}{\partial \mathbf{w}} - \dfrac{\partial (2Y^TX\mathbf{w})}{\partial \mathbf{w}} + \dfrac{\partial (\mathbf{w}^TX^TX\mathbf{w})}{\partial \mathbf{w}}$

Biraz daha ipucu isterseniz her bir terimi toplama sembolÃ¼ ile yazÄ±p daha aÃ§Ä±k ÅŸekilde gÃ¶rmeyi deneyin, unutmayÄ±n $\mathbf{w}_1$'e gÃ¶re tÃ¼rev alÄ±rken diÄŸer tÃ¼m $\mathbf{w}_2$, $\mathbf{w}_3$, ..., $\mathbf{w}_D$ deÄŸerleri sabit gibi davranacak. Yani $\mathbf{w}$ iÃ§erse bile $\mathbf{w}_1$ iÃ§ermeyen her ÅŸeyin tÃ¼revi $0$ olacak, aynÄ± ÅŸekilde her bir $\mathbf{w}_i$'ye gÃ¶re tÃ¼rev alÄ±rken diÄŸer tÃ¼m $\mathbf{w}_j$'ler sabit gibi davranacak.

Toplama sembolÃ¼ olarak nasÄ±l yazarÄ±m derseniz bu konuda bir ipucu:

```math
Y^TX\mathbf{w} = âˆ‘_{i=1}^{N} âˆ‘_{j=1}^{D} Y_i \times \mathbf{w}_j \times X_{ij}
```

EÄŸer illa ben uÄŸraÅŸmak istemiyorum derseniz, asla ama asla tavsiye etmemek ile beraber, bu linkten hangi terimin tÃ¼revinin nasÄ±l alÄ±ndÄ±ÄŸÄ±na bakabilirsiniz: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf

En sonunda hesapladÄ±ÄŸÄ±nÄ±z gradyanÄ± $\mathbf{0}$'a eÅŸitleyip $\mathbf{w}^*$ bulmayÄ± unutmayÄ±n :)

Herkese kolaylÄ±klar :)