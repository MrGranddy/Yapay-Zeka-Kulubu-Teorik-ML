# Least Squares Method

## ns繹z

Merhaba arkadalar, girizgah 繹nemli olduu i癟in bu haftaki materyali kendim haz覺rlamaya karar verdim, bu sayede eitim grubunun devam覺nda kar覺m覺za 癟覺kacak kavramlar覺 size daha iyi aktarabileceimi d羹羹nd羹m.

Materyalin genelinde $\mathcal{D} = \{x^{(i)}\}_1^N, x^{(i)} \in \mathbb{R}^2$ 繹rnei gibi **matematiksel notasyon** kullan覺m覺 ile s覺k s覺k kar覺laacaks覺n覺z, her ne kadar basit kavramlar覺 anlat覺rken pek ihtiyac覺m覺z olmasa da g羹n羹n sonunda matematiksel notasyon okur yazarl覺覺m覺z olmas覺 繹nemli, bir noktadan sonra kavramlar覺 baka ekilde anlatman覺n pratik bir yolu kalm覺yor. Bu yaz覺da ayn覺 zamanda size s覺k癟a kullanaca覺m覺z baz覺 notasyonlar覺 da g繹stermeye 癟al覺aca覺m, sizden ricam onlar覺 da as覺l materyal gibi 繹nemseyip kafan覺zda oturtmaya 癟al覺man覺z.

## Nedir?

**Least Squares Method**, genel olarak yapay zeka temal覺 bir癟ok dersin olmazsa olmazlar覺ndan olan bir y繹ntem. 襤leride kar覺m覺za 癟覺kacak bir癟ok kavram覺 basit bir ekilde kullanmam覺z a癟覺s覺ndan g羹zel bir 繹rnek.

Elinizde $D$ boyutlu $N$ adet vekt繹r olduunu d羹羹n羹n. rnein $D = 2$ ve $N = 100$ ise bu elimizde $(x, y)$ gibi g繹sterebileceimiz $100$ adet nokta var demek. Burada 繹nemli bir detay, 癟ou zaman **vekt繹r** ve **nokta** terimleri birbirinin yerine kullan覺labiliyor, yani basit癟e elimizde hepimizin bildii 2 boyutlu bir koordinat sistemindeki noktalar var diyebiliriz.

![Least Squares Method rnek Data](lsm_example_data.png)

Bunu g繹stermenin alternatif bir yolu da elimizde $\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}^2$ olmak 羹zere bir veri k羹mesi (**dataset**) olduunu s繹ylemek. $D$ ve $N$'i genel hali ile b覺rakmak istersek ise $\mathcal{D} = \{x^{(i)}\}_{i=1}^{N}, x^{(i)} \in \mathbb{R}^D$ diyebiliriz.

Buraya kadar her ey g羹zel, peki bu t羹r bir veri k羹mesi ile kar覺laabileceimiz ger癟ek bir senaryo ne olabilir? Yine 2 boyutlu bir 繹rnek 羹st羹nden gidelim, 繹rnein 襤stanbul'daki belli say覺da arsa i癟in elimizde arsan覺n alan覺 ve sat覺 fiyat覺 olsun.

![Least Squares Method rnek Data](lsm_arsa_ornegi.png)

Say覺lara tak覺lmamaya 癟al覺覺n :) Peki, elimizde bu data mevcut ve elimize yeni bir arsa geldii zaman fiyat覺n覺 tahmin etmek istiyoruz, bu durumda ne yapabiliriz? 襤te burada **Least Squares Method** devreye giriyor.

![Least Squares Method rnek Data](lsm_arsa_ornegi_with_line.png)

Eer elimizde k覺rm覺z覺 ile g繹sterilen dorunun denklemi varsa, yeni bir arsa i癟in sadece arsan覺n alan覺n覺 bilerek bir fiyat tahmini yapabiliriz. Least Squares Method bizim bu doruyu bulmam覺z覺 sal覺yor.

Formal bir tan覺m yapacak olursak: Elimizde $\mathcal{D} = \{x^{(i)}\}_{i=1}^{N}, x^{(i)} \in \mathbb{R}^{D+1}$ veri k羹mesi olsun, yani elimizdeki her bir vekt繹r $D+1$ boyutlu, 繹rnein vekt繹rlerimizin her bir eleman覺 **alan**, **fiyat**, **en**, **boy** gibi 繹zellikleri tan覺ml覺yor olabilir, $x^{(i)} \in \mathbb{R}^{D+1}$ dediimizde ise bu elemanlar覺n birer reel say覺 olduunu ifade ediyor.

Daha a癟覺k s繹ylemek gerekirse $x^{(1)}$, $x^{(2)}$, ..., $x^{(N)}$ gibi isimlendirilen, elimizdeki her bir vekt繹r $D+1$ adet reel say覺 ile ifade ediliyor. Az 繹nce $D$ derken imdi $D+1$ demem kafan覺z覺 kar覺t覺rmas覺n, bir sonraki ad覺m覺 daha anla覺l覺r k覺lmak i癟in yapt覺覺m bir deiiklik.

imdi her bir vekt繹r i癟in bu $D+1$ adet reel say覺dan bir tanesini kenara ay覺ral覺m (繹rnein arsan覺n fiyat覺) ve bunlardan yeni bir k羹me olutural覺m:

$\mathcal{Y} = \{y^{(i)}\}_{i=1}^{N}, y^{(i)} \in \mathbb{R}$

Her bir vekt繹rden sadece tek bir eleman覺 ay覺rd覺覺m覺z i癟in yeni k羹memizdeki elemanlar da sadece tek boyutlu, bu y羹zden $y^{(i)} \in \mathbb{R}$ diyebiliyoruz. rnein 繹nceden elimizde (**alan**, **fiyat**, **en**, **boy**) eklinde vekt繹rler varken, imdi elimizde (**alan**, **en**, **boy**) ve ( **fiyat** ) olmak 羹zere iki ayr覺 癟eit vekt繹r var.

Art覺k elimizde iki adet k羹me var:

$\mathcal{D} = \{x^{(i)}\}_{i=1}^{N}, x^{(i)} \in \mathbb{R}^{D}$ ve $\mathcal{Y} = \{y^{(i)}\}_{i=1}^{N}, y^{(i)} \in \mathbb{R}$

襤te bu y羹zden $D$ yerine $D+1$ demitim :) Kar覺覺k g繹r羹n羹yor olabilir ama asl覺nda 癟ok basit, elimizdeki veri k羹mesini iki par癟aya ay覺rd覺k, bir tanesi 繹zellikleri i癟eriyor, dieri ise fiyatlar覺 i癟eriyor ve elbette $x^{(i)}$ 繹zelliklerine kar覺l覺k gelen $y^{(i)}$ fiyat覺 ile eleiyor, yani $x^{(1)}$ 繹zelliklerine sahip arsan覺n fiyat覺 $y^{(1)}$, $x^{(2)}$ 繹zelliklerine sahip arsan覺n fiyat覺 $y^{(2)}$ ve b繹yle devam ediyor.

Deiken isimleri ve bir t覺k karma覺k baz覺 matematiksel zamazingolar ile kafan覺z覺 iirdiim i癟in kusuruma bakmay覺n :) Ama inan覺n her ey sizin iyiliiniz i癟in.

imdi her eyin yerli yerine oturmas覺 i癟in as覺l 繹rneimizde bu yeni k羹meleri tan覺mlayal覺m. ncelikle elimizdeki veri k羹mesi:

![Least Squares Method rnek Data](lsm_arsa_ornegi.png)

Bu veri k羹mesini $\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}^2$ eklinde tan覺mlam覺t覺k, az 繹nce yapt覺覺m覺z gibi iki par癟aya ay覺rd覺覺m覺zda ise elimizde:

$\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}$ ve $\mathcal{Y} = \{y^{(i)}\}_{i=1}^{100}, y^{(i)} \in \mathbb{R}$

eklinde iki k羹me oluyor. Yani bir k羹me sadece arsa alanlar覺n覺 i癟erirken, dieri ise sadece fiyatlar覺 i癟eriyor. Art覺k problemimizi daha kolay bir ekilde ifade edebiliriz, herhangi bir $x^{(i)}$ i癟in $y^{(i)}$'yi tahmin etmek istiyoruz, yani herhangi bir arsan覺n alan覺 bilindiinde fiyat覺n覺 tahmin etmek istiyoruz. Tabii ki $x^{(i)}$ 1 boyutlu olmak zorunda deildi, alan bilgisinin yan覺nda daha bir s羹r羹 繹zellii de i癟inde bar覺nd覺rabilirdi, bu y羹zden genel senaryo i癟in $x^{(i)} \in \mathbb{R}^D$ diyoruz. Ama eer bu 繹rnekte $D$'yi 1'den b羹y羹k bir say覺 belirleseydik, g繹rselletirmemiz olduk癟a zorlaacakt覺, hadi 2 olsa yine bir ekilde yapard覺k ama 3'ten sonras覺n覺 g繹rsel olarak kafas覺nda canland覺ran varsa helal olsun :) (Unutmay覺n $x$'in 3 eleman覺 yan覺nda bir de kar覺l覺k olarak $y$'nin 1 eleman覺 var, yani 4 boyutlu bir uzaydan bahsediyoruz)

imdi elle tutulur eylere geri d繹nelim, ne demitik:

![Least Squares Method rnek Data](lsm_arsa_ornegi_with_line.png)

K覺rm覺z覺 doruyu 癟izersek, problemi 癟繹zeriz. Ka覺t kalem ile yapmas覺 olduk癟a kolay, ama bahsettiimiz $D$'nin $1$ olmad覺覺 senaryolar i癟in biraz daha matematiksel d羹羹nelim. En nihayetinde elde etmek istediimiz ey, herhangi bir arsa alan覺 i癟in bu arsan覺n fiyat覺n覺 kusursuz bir ekilde tahmin etmek, elbette arsan覺n sadece alan覺n覺 biliyorsak bu olduk癟a zor bir i, hatta daha fazla bilgimiz olsa bile, hayatta her ey rasyonel deil, birileri arsas覺n覺 癟ok uzuca veya 癟ok pahal覺ya sat覺yor olabilir, ger癟ek hayatta kar覺m覺za 癟覺kan bu t羹r keyfi oynamalara __noise__ yani __g羹r羹lt羹__ deriz. Yani kusursuz tahmin i癟in 癟ok da heveslenmemekte fayda var, ama biz yine de elimizden geleni yapal覺m.

Daha da basitletirmek istersek, elimizde $1$ adet reel say覺 var ve bir buna kar覺l覺k baka $1$ reel say覺y覺 tahmin etmek istiyoruz. Bu bize 癟ok iyi bildiimiz bir yap覺y覺 an覺msat覺yor: __fonksiyonlar__.

襤deal bir d羹nyada sihirli bir ekilde 繹yle bir $f(x)$ fonksiyonu elde edebiliriz ki, $f(x^{(1)}) = y^{(1)}$ olur, $f(x^{(2)}) = y^{(2)}$ olur, $f(x^{(3)}) = y^{(3)}$ olur ve b繹yle devam eder. Elimizde bu fonksiyon varsa, fiyat覺n覺 bilmeyip alan覺n覺 bildiimiz herhangi bir arsa i癟in, bu fonksiyona alan覺 verdiimizde fiyat覺n覺 tahmin edebiliriz. Peki bu fonksiyonu nas覺l elde edeceiz?

Yine bildiimiz basit ger癟eklere d繹nelim, k覺rm覺z覺 doruyu hat覺rlay覺n:

![Least Squares Method rnek Data](lsm_arsa_ornegi_with_line.png)

2 boyutlu bu dorunu denkleminin neye benzeyeceini 癟ok iyi biliyoruz: $f(x) = a \times x + b$

Yani asl覺nda bulmam覺z gereken $2$ adet deiken var, $a$ ve $b$. 襤nsan dili ile ifade etmemiz gerekirse eim ve y eksenini kestii nokta. imdi gelin makine 繹renmesindeki en temel kavramlardan bir tanesini tan覺yal覺m ve 癟繹z羹me bir ad覺m daha yaklaal覺m: __Error Function__ yani __hata fonksiyonu__. 

Makine 繹renmesinde uygulad覺覺m覺z en temel stratejilerden biri, 癟ok k繹t羹 bir tahmin fonksiyonunu al覺p ad覺m ad覺m iyiletirmek. Ne mi demek istiyorum? Diyelim elimizde arsa fiyat覺 tahmini i癟in bir fonksiyon var ve ad覺 $g(x)$. 

Peki elimizde $g(x)$ var, ve ger癟ekten de ona bir arsan覺n alan覺n覺 s繹ylediimizde bize fiyatla ilgili bir tahmin yap覺yor. Peki $g(x)$'in genel olarak ne kadar baar覺l覺 olduunu nas覺l anlayaca覺z? Evet elimizde az say覺da 繹rnek varsa tek tek hesaplay覺p bakabiliriz ama tahmin edeceiniz gibi bu pek de iyi bir fikir deil.

u iimizi 癟ok daha kolaylat覺r覺rd覺, 繹yle bir fonksiyon var ki, ben elimdeki $g(x)$ fonksiyonunun yapt覺覺 tahminleri ona verince bana pozitif bir reel say覺 d繹nd羹r羹yor, bu say覺 da $g(x)$'in ne kadar baar覺l覺 olduunu g繹steriyor. Bu say覺 ne kadar k羹癟羹kse, $g(x)$ o kadar baar覺l覺 demek. Bu say覺ya __error__ (__hata__) diyoruz, yani $g(x)$'in yapt覺覺 tahminlerin ne kadar yanl覺 olduunu g繹steren bir say覺.

Diyelim ki $\mathcal{D} = \{x^{(i)}\}_{i=1}^{100}, x^{(i)} \in \mathbb{R}$ veri k羹mesindeki t羹m $x^{(i)}$'leri $g(x)$'e veriyoruz ve hepsi i癟in bir tahmin al覺yoruz, bu tahminleri $\hat{y}^{(i)}$ eklinde isimlendirelim. Yani $g(x^{(i)}) = \hat{y}^{(i)}$. Dikkat edin $\hat{y}$'nin 羹zerinde bir apka var.

imdi as覺l noktaya geliyoruz, $\hat{y}^{(i)}$ bizim tahminimiz, $y^{(i)}$ ise ger癟ek deer, t羹m $i$'ler i癟in bu iki deer eit olsayd覺, bu $g(x)$'in kusursuz 癟al覺t覺覺n覺 g繹sterirdi, yani bu __hata__ dediimiz eyi hesaplaman覺n bir yolunu bilseydik, hatan覺n $0$ olmas覺n覺 beklerdik.

O zaman 繹yle bir ey yapal覺m, her bir $i$ i癟in $y^{(i)} - \hat{y}^{(i)}$ hesaplayal覺m ve bunlar覺 toplayal覺m, $g(x)$'in kusursuz 癟al覺t覺覺 senaryoda bu deer ger癟ekten de 0 gelirdi. Ama burada bir ey ters, deerlerin eit olmad覺覺 senaryoda $\hat{y}^{(i)}$ b羹y羹d羹k癟e toplam deer k羹癟羹l羹r, yani hata azal覺r, ama durum bu deil, 癟羹nk羹 $\hat{y}^{(i)}$ b羹y羹yorsa $y^{(i)}$ ile aras覺ndaki fark a癟覺l覺yor demek! Bu y羹zden $y^{(i)} - \hat{y}^{(i)}$'nin mutlak deerini al覺p toplamak daha mant覺kl覺, yani $|y^{(i)} - \hat{y}^{(i)}|$, ite bu bize ger癟ekten de tahmin edilen fiyatlar ve ger癟ek fiyatlar覺n birbirine ne kadar benzediini g繹sterir. Benzerlerse deer k羹癟羹k olur, deillerse deer b羹y羹k olur. Ve fark edeceiniz 羹zere bizim ede etmek istediimiz __hata fonksiyonu__'nun tam olarak bunu yapmas覺 gerekiyor! O zaman fonksiyonumuzu bulduk.

$E = \sum_{i=1}^{100} |y^{(i)} - \hat{y}^{(i)}|$

Burada $E$ __error__'覺n ba harfinden geliyor. Peki imdi hat覺rlay覺n $g(x) = a \times x + b$ formunda demitik, yani her farkl覺 $a$ ve $b$ deeri i癟in $g(x)$ farkl覺 bir doruyu ifade ediyor. Biz bu farkl覺 dorular aras覺nda bize en d羹羹k hatay覺 veren $g(x)$'i istiyoruz. O zaman hata fonksiyonunu $a$ ve $b$ cinsinden yazabiliriz:

$E(a, b) = \sum_{i=1}^{100} |y^{(i)} - (a \times x^{(i)} + b)|$

Burada $E$'nin yan覺na $a$ ve $b$ yazd覺k, 癟羹nk羹 $E$'nin $a$ ve $b$'ye bal覺 olduunu belirtmek istiyoruz, en nihayetinde datam覺z yani arsalara kar覺l覺k gelen alan ve fiyat ikilileri deimiyor, deien ey $a$ ve $b$ yani datam覺za uygun olacak ekilde deitireceimiz deikenler.

Bu noktada unu belirtmekte fayda var, makine 繹renmesi gibi matematik temeli youn alanlarda, en basit y繹ntem i癟in bile t羹m ilemleri, denklemleri, deikenleri s羹rekli akl覺m覺zda tutmam覺z 癟ok m羹mk羹n olmayabilir. O y羹zden bu noktada batan sona t羹m aamalar akl覺n覺zda deilse endie etmeyin. nemli olan aamalar aras覺 her bir ufak ge癟ii anlamak, eer u noktaya kadar her bir ad覺m覺 anlad覺ysan覺z, bata elimizde olan ey ile u an elimizde olan ey aras覺nda direkt bir balant覺 g繹rmenize gerek yok, en nihayetinde her bir ad覺mdan tek tek eminseniz, u an elinizdeki ey doru demektir.

Peki dediimiz gibi burada verimiz sabit, yani elimizde ayn覺 100 adet arsa, arsalar覺n alanlar覺 ve fiyatlar覺 var, yani asl覺nda __hata fonksiyonu__'muz i癟in $x^{(i)}$ ve $y^{(i)}$ sabit. __Hata fonksiyonu__ sadece ve sadece $a$ ve $b$ deikenlerine bal覺!

Bu iimizi 癟ok ama 癟ok kolaylat覺r覺yor. Elimizde bir $E(a, b)$ fonksiyonu var ve bu fonksiyonun deerini olabilecek en k羹癟羹k deer yapan $a$ ve $b$ deikenlerini bulmak istiyoruz. Ne yapmam覺z gerektiini biliyorsunuz :)

Eer $E(a, b)$'nin $a$ ve $b$'ye g繹re t羹revini al覺p 0'a eitleyip $a$ ve $b$ i癟in denklemi 癟繹zersek, $E(a, b)$'nin $a$ ve $b$'ye g繹re minimum olduu noktay覺 bulmu oluruz. K覺saca hat覺rlayal覺m:

$h(a, b) = 5a^2 + 3ab + 2b^2 + 3a + 5b + 1$ olsun. $h(a, b)$'nin $a$'ya g繹re t羹revini alal覺m:

$\frac{\partial h(a, b)}{\partial a} = 10a + 3b + 3$ imdi 癟覺kan ifadeyi $0$'a eitleyelim:

$10a + 3b + 3 = 0$ buradan $a = -\frac{3b}{10} - \frac{3}{10}$ elde ederiz. imdi $h(a, b)$'nin $b$'ye g繹re t羹revini alal覺m:

$\frac{\partial h(a, b)}{\partial b} = 3a + 4b + 5$ imdi 癟覺kan ifadeyi $0$'a eitleyelim:

$3a + 4b + 5 = 0$ buradan $b = -\frac{3a}{4} - \frac{5}{4}$ elde ederiz. imdi sistemimizi 癟繹zelim:

$a = -\frac{3b}{10} - \frac{3}{10}$

$b = -\frac{3a}{4} - \frac{5}{4}$

Maalesef bunu ad覺m ad覺m yapmayaca覺m 癟羹nk羹 deerler 癟ok sa癟ma geliyor :) Ama 2 bilinmeyenli 2 denklemi 癟繹zmek i癟in bildiimiz y繹ntemleri kullanabiliriz, sonu癟 ise:

$a \approx 0.0968$ ve $b \approx -1.3226$ olacak.

Tamamen ayn覺 mant覺k $E(a,b)$ fonksiyonumuz i癟in de ge癟erli. Ama yine 繹n羹m羹zde ufak bir engel var. Pek tabii ki bunu yapman覺n yollar覺 var ama $E(a, b)$'yi mutlak deerli bir fonksiyon olarak tan覺mlad覺k ve bu minimumunu bulmak i癟in 癟ok g羹zel bir fonksiyon deil. 襤te bu noktada __Least Squares Method__'un ad覺nda da olan ufak bir deiiklik ile iimizi kolaylat覺rabiliriz __squares__ yani __kareler__.

Hat覺rlarsan覺z mutlak deeri kullanma sebebimiz rastgele farklara bakmaktansa iki deerin birbirine ne kadar uzak olduunu 繹l癟mekti, bunu yapmak i癟in illa mutlak deer kullanmak zorunda deiliz, unu d羹羹n羹n $(y^{(i)} - \hat{y}^{(i)})^2$ deseydik, yine istediimiz 繹zellikleri salamaz m覺yd覺? Evet salard覺, 癟羹nk羹 $(y^{(i)} - \hat{y}^{(i)})^2$'nin deeri her zaman pozitif ve deerler birbirine ne kadar uzaksa o kadar b羹y羹k olur. Bu y羹zden __Least Squares Method__'da __squares__ yani __kareler__ kullan覺l覺yor. O zaman __hata fonksiyonu__'muzu buna g繹re g羹ncelleyelim:

$E(a, b) = \sum_{i=1}^{100} (y^{(i)} - (a \times x^{(i)} + b))^2$.

Peki, teoride bu ifadenin t羹revini alabileceimizi biliyoruz, daha sonras覺nda $a$'ya ve $b$'ye g繹re t羹revleri $0$'a eitlememiz gerektiini de biliyoruz. Asl覺nda u an elinize ka覺t kalemi al覺p $a$ ve $b$ i癟in $x^{(i)}$ ve $y^{(i)}$'e g繹re birer denklem 癟覺karabilirsiniz, fakat bu bizi 癟ok urat覺r覺r. Gelin bir ad覺m geri gidelim:

$E(a, b) = \sum_{i=1}^{100} (y^{(i)} - \hat{y}^{(i)})^2$ demitik. Hadi bir ad覺m daha geri gidip $100$ yerine $N$ yazal覺m:

$E(a, b) = \sum_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$.

Elimizde olduk癟a yal覺n bir ifade var. Bunu daha da yal覺nlat覺rman覺n bir yolu olabilir mi? Mesela u toplam sembol羹nden bir kurtulsak? Haf覺zan覺z覺 tazelemek i癟in bir 繹rnek d羹羹nelim, elimde bir vekt繹r olsun:

$\mathbf{v} = \begin{bmatrix} a \\ b \\ c \\ d \\ e \end{bmatrix}$

Bu vekt繹r羹 kendisi ile skaler 癟arp覺m yaparsak ne olur?

$\mathbf{v} \cdot \mathbf{v} = \mathbf{v}^T\mathbf{v} = a^2 + b^2 + c^2 + d^2 + e^2$

Yani her bir eleman覺n karelerinin toplam覺. O halde benim elimde elemanlar覺 $y^{(i)} - \hat{y}^{(i)}$ olan bir vekt繹r olsa:

$\mathbf{v} = \begin{bmatrix} y^{(1)} - \hat{y}^{(1)} \\ y^{(2)} - \hat{y}^{(2)} \\ \vdots \\ y^{(N)} - \hat{y}^{(N)} \end{bmatrix}$ ve bu vekt繹r羹 kendisi ile skaler 癟arpsam:

$\mathbf{v} \cdot \mathbf{v} = \mathbf{v}^T\mathbf{v} = \sum_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$ 仁仁仁仁

Hatta ve hatta $Y$ ve $\hat{Y}$ olmak 羹zere iki vekt繹r羹m羹z olsa:

$Y = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)} \end{bmatrix}$ ve $\hat{Y} = \begin{bmatrix} \hat{y}^{(1)} \\ \hat{y}^{(2)} \\ \vdots \\ \hat{y}^{(N)} \end{bmatrix}$

$\mathbf{v} = Y - \hat{Y}$ yani:

$\mathbf{v}^T\mathbf{v} = (Y - \hat{Y})^T(Y - \hat{Y}) = \sum_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$ 仁仁仁仁仁仁

Toparlarsak:

$E(a, b) = \mathbf{v}^T\mathbf{v} = (Y - \hat{Y})^T(Y - \hat{Y}) = \sum_{i=1}^{N} (y^{(i)} - \hat{y}^{(i)})^2$

imdi ileri bir ad覺m daha ileriye ta覺yal覺m, en bata hata fonksiyonumuzun $a$ ve $b$'ye bal覺 olmas覺n覺n sebebi, $g(x) = a \times x + b$ olmas覺yd覺. Bu da en baa d繹nersek $D$ yani vekt繹rlerimizin boyutu $1$ olduu i癟indi. Peki ya $D$'yi de tekrar eski haline getirseydik, mesela her $x^{(i)} \in \mathbb{R}^D$ yani $D$ boyutlu birer vekt繹r olsayd覺? O zaman genel senaryoda:

$x^{(i)} = \begin{bmatrix} x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_D^{(i)} \end{bmatrix}$ olurdu.

Yani art覺k sadece arsan覺n alan覺 deil, arsa ile ilgili 癟eit 癟eit bilgilere sahibiz. Peki ayn覺 ekilde tahmin fonksiyonumuzu da genelleyemez miyiz? $g(x) = a \times x + b$ yerine:

$g(x) = w_1 \times x_1 + w_2 \times x_2 + ... + w_D \times x_D + b$

desek, her bir deere kar覺l覺k bir katsay覺 olan asl覺nda olduk癟a basit bir fonksiyon tasla覺m覺z olurdu. Tek bir 癟arp覺m yapmak yerine $D$ 癟arp覺m yap覺p toplamaktan baka hi癟bir fark yok. $g(x) = a \times x + b$'de $x$'e bal覺 olmayan $b$'yi de olduu gibi b覺rakt覺k. Yine matris 癟arp覺m覺 notasyonumuzu kullanarak bunu da basitletirebiliriz:

$\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_D \end{bmatrix}$ ve $\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_D \end{bmatrix}$ dersek:

$g(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b$ olur.

imdi bu fonksiyonu kullanarak $\hat{y}^{(i)}$'yi yazal覺m:

$\hat{y}^{(i)} = g(x^{(i)}) = \mathbf{w}^T\mathbf{x}^{(i)} + b$

Bu noktada $b$ bizler i癟in 繹nemli bir katk覺, fakat ilemleri biraz da olsa kar覺t覺racak, o y羹zden imdilik onu iin i癟inden 癟覺kartal覺m, devam覺nda $b$'yi 癟覺karmadan yapmay覺 da size b覺rak覺yorum. imdi $\hat{y}^{(i)}$'yi yazal覺m:

$\hat{y}^{(i)} = \mathbf{w}^T\mathbf{x}^{(i)}$ imdi $\hat{Y}$ vekt繹r羹n羹 de tekrar elden ge癟irelim:

$\hat{Y} = \begin{bmatrix} \hat{y}^{(1)} \\ \hat{y}^{(2)} \\ \vdots \\ \hat{y}^{(N)} \end{bmatrix} = \begin{bmatrix} \mathbf{w}^T\mathbf{x}^{(1)} \\ \mathbf{w}^T\mathbf{x}^{(2)} \\ \vdots \\ \mathbf{w}^T\mathbf{x}^{(N)} \end{bmatrix}$

Bu noktada asl覺nda her eyi 癟ok 癟ok daha basitletireceimiz bir ad覺m var. Bu ad覺m matris 癟arp覺m覺na aina olanlar覺n覺z i癟in bariz olabilir, ama deilse de sorun deil, bir sonraki ad覺m sizin i癟in a癟覺k deilse bunu 癟覺karmay覺 da size bir 繹dev olarak b覺rak覺yorum :)

yle bir $X$ matrisi tan覺mlayabiliriz ki:

$X = \begin{bmatrix} \mathbf{x}^{(1)} \\ \mathbf{x}^{(2)} \\ \vdots \\ \mathbf{x}^{(N)} \end{bmatrix}$ burada her bir $\mathbf{x}^{(i)}$ birer sat覺r vekt繹r羹. Bu durumda $\hat{Y}$'yi u ekilde yazabiliriz:

$\hat{Y} = X\mathbf{w}$

B羹t羹n bu zorlu uralar覺n ve emeklerimizin kar覺l覺覺nda, genel $N$ ve $D$ i癟in, hayal etmeye bile c羹ret edemeyeceimiz basitlikte bir ifade elde ettik, son olarak hata fonksiyonumuzu da bu ifadeyi kullanarak yazal覺m:

$E(\mathbf{w}) = (Y - \hat{Y})^T(Y - \hat{Y}) = (Y - X\mathbf{w})^T(Y - X\mathbf{w}) = \sum_{i=1}^{N} (y^{(i)} - \mathbf{w}^T\mathbf{x}^{(i)})^2$

Fark ettiiniz gibi art覺k __hata fonksiyonu__ $a$ ve $b$'ye deil, $\mathbf{w}$'ye bal覺, 癟羹nk羹 art覺k $g(x) = \mathbf{w}^T\mathbf{x}$ ve fonksiyonu deitiren parametrelerimiz $\mathbf{w}$ vekt繹r羹nde. Bununla da s覺n覺rl覺 kalmayal覺m, $(Y - X\mathbf{w})^T(Y - X\mathbf{w})$ ifadesini a癟al覺m:

$E(\mathbf{w}) = (Y - X\mathbf{w})^T(Y - X\mathbf{w}) = Y^TY - Y^TX\mathbf{w} - \mathbf{w}^TX^TY + \mathbf{w}^TX^TX\mathbf{w}$

Burada kulland覺覺m覺z kurallar:

1. $(AB)^T = B^TA^T$ hatta $(ABC)^T = C^TB^TA^T$ eklinde genelleyebiliriz
2. $(A + B)^T = A^T + B^T$

襤imizi bir t覺k daha kolaylat覺rmak i癟in bir numara daha var, hat覺rlarsan覺z $E(\mathbf{w})$ bize bir __hata__ deeri d繹nd羹r羹yor ve bu deer sadece skaler bir reel say覺. Bu da demek oluyor ki $E(\mathbf{w})$'yi elde etmek i癟in toplad覺覺m覺z $4$ ayr覺 terim de en nihayetinde birer skaler olmak zorunda. Skaler bir say覺 i癟in a覺rt覺c覺 olmayacak ekilde transpozu kendisine eittir, 繹rnein $5^T = 5$. O halde $E(\mathbf{w})$'yi oluturan her bir terimin transpozu kendisine eit, burada g繹rm羹 olabileceiniz 羹zere $(\mathbf{w}^TX^TY)^T = Y^TX\mathbf{w}$ ve bu terimden zaten bir tane mevcut, o halde ikisini toplay覺p __hata fonksiyonu__'nu son haline getirebiliriz:

$E(\mathbf{w}) = (Y - X\mathbf{w})^T(Y - X\mathbf{w})$

$= Y^TY - Y^TX\mathbf{w} - \mathbf{w}^TX^TY + \mathbf{w}^TX^TX\mathbf{w}$

$= Y^TY - Y^TX\mathbf{w} - (\mathbf{w}^TX^TY)^T + \mathbf{w}^TX^TX\mathbf{w}$

$= Y^TY - Y^TX\mathbf{w} - Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w}$

$= Y^TY - 2Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w}$

Yani son olarak elimizdeki her eyi 繹zetlemek gerekirse:

$E(\mathbf{w}) = Y^TY - 2Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w}$

ve eer $\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = 0$'覺 癟繹zersek. Buradan hatay覺 minimum yapan $\mathbf{w}^*$'yi bulmu oluruz. st羹ndeki * (asterisk) de art覺k bunun herhangi bir $\mathbf{w}$ deil, hatay覺 minimum yapan $\mathbf{w}$ olduunu belirtmek i癟in kullan覺lan bir iaret.

Elbette $\mathbf{w}^*$, $Y$ ve $X$'e bal覺 bir deer 癟覺kacak. Bu noktada iin as覺l keyifli taraf覺 olan optimal $\mathbf{w}^*$'yi bulma iini size b覺rak覺yorum. Evet baka korkutucu g繹z羹kebilir ama bir kere baard覺覺n覺zda ba覺ml覺s覺 olacaks覺n覺z. Tabii ki sizi bu konuda yaln覺z b覺rakmamak i癟in birka癟 ipucu ve kaynak ile ii noktal覺yorum. Bu aamadan sonra ise belki daha da keyifli olacak olan ellerimizle haz覺rlad覺覺m覺z bu metodun kodunu yazaca覺z ve sonu癟lar覺 g繹receiz, bunun detaylar覺 i癟in Whatsapp grubunu takipte kal覺n :)

## 襤pu癟lar覺

$\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}}$ demek, skaler deerli bir fonksiyonun bir vekt繹re g繹re gradyan覺 demek.

Gradyan覺n ne olduu ve nas覺l al覺nd覺覺 ile ilgili kaynak: https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient

Fiziksel a癟覺klamalar覺 ge癟ip direkt olarak nas覺l hesapland覺覺na bakabilirsiniz, asl覺nda 癟ok basit $\mathbf{w}$'deki her bir eleman i癟in, tek tek o elemana g繹re t羹m ifadenin t羹revini al覺n ve bunlar覺 bir vekt繹r olarak alt alta yaz覺n, yani:

$\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \begin{bmatrix} \dfrac{\partial E(\mathbf{w})}{\partial w_1} \\ \dfrac{\partial E(\mathbf{w})}{\partial w_2} \\ \vdots \\ \dfrac{\partial E(\mathbf{w})}{\partial w_D} \end{bmatrix}$

Tabii ki i bunla bitmeyecek, gradyan da t羹rev gibi toplamaya da覺labilen bir ilem o y羹zden ii biraz basitletirmek i癟in, unu da size verebilirim:

$\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \dfrac{\partial (Y^TY - 2Y^TX\mathbf{w} + \mathbf{w}^TX^TX\mathbf{w})}{\partial \mathbf{w}} = \dfrac{\partial (Y^TY)}{\partial \mathbf{w}} - \dfrac{\partial (2Y^TX\mathbf{w})}{\partial \mathbf{w}} + \dfrac{\partial (\mathbf{w}^TX^TX\mathbf{w})}{\partial \mathbf{w}}$

Biraz daha ipucu isterseniz her bir terimi toplama sembol羹 ile yaz覺p daha a癟覺k ekilde g繹rmeyi deneyin, unutmay覺n $\mathbf{w}_1$'e g繹re t羹rev al覺rken dier t羹m $\mathbf{w}_2$, $\mathbf{w}_3$, ..., $\mathbf{w}_D$ deerleri sabit gibi davranacak. Yani $\mathbf{w}$ i癟erse bile $\mathbf{w}_1$ i癟ermeyen her eyin t羹revi $0$ olacak, ayn覺 ekilde her bir $\mathbf{w}_i$'ye g繹re t羹rev al覺rken dier t羹m $\mathbf{w}_j$'ler sabit gibi davranacak.

Toplama sembol羹 olarak nas覺l yazar覺m derseniz bu konuda bir ipucu:

$Y^TX\mathbf{w} = \sum_{i=1}^{N} \sum_{j=1}^{D} Y_i \times \mathbf{w}_j \times X_{ij}$

Eer illa ben uramak istemiyorum derseniz, asla ama asla tavsiye etmemek ile beraber, bu linkten hangi terimin t羹revinin nas覺l al覺nd覺覺na bakabilirsiniz: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf

En sonunda hesaplad覺覺n覺z gradyan覺 $\mathbf{0}$'a eitleyip $\mathbf{w}^*$ bulmay覺 unutmay覺n :)

Herkese kolayl覺klar :)